{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.16.2)\n",
      "Requirement already satisfied: keras in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.8.0)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras nltk numpy json pickle ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1668892896.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -m nltk.downloader punkt\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m nltk.downloader punkt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neha/nltk_data/tokenizers/punkt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.find('tokenizers/punkt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/neha/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mUntitled-6.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m intent \u001b[39min\u001b[39;00m intents[\u001b[39m\"\u001b[39m\u001b[39mintents\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=25'>26</a>\u001b[0m     \u001b[39mfor\u001b[39;00m pattern \u001b[39min\u001b[39;00m intent[\u001b[39m\"\u001b[39m\u001b[39mpatterns\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=26'>27</a>\u001b[0m         word_list \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(pattern)\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=27'>28</a>\u001b[0m         words\u001b[39m.\u001b[39mextend(word_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=28'>29</a>\u001b[0m         documents\u001b[39m.\u001b[39mappend((word_list, intent[\u001b[39m\"\u001b[39m\u001b[39mtag\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/neha/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Lists to store words, classes, and documents\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = [\"?\", \"!\", \".\", \",\"]\n",
    "\n",
    "# Tokenize words and prepare dataset\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent[\"tag\"]))\n",
    "        if intent[\"tag\"] not in classes:\n",
    "            classes.append(intent[\"tag\"])\n",
    "\n",
    "# Lemmatize and sort words\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "# Save words and classes using pickle\n",
    "pickle.dump(words, open(\"words.pkl\", \"wb\"))\n",
    "pickle.dump(classes, open(\"classes.pkl\", \"wb\"))\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/neha/Library/Python/3.9/lib/python/site-packages (4.27.2)\n",
      "Requirement already satisfied: torch in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.61.1-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: ipywidgets in /Users/neha/Library/Python/3.9/lib/python/site-packages (8.1.5)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.37-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from langchain) (3.11.11)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.33 (from langchain)\n",
      "  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.6-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from openai) (4.8.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /Users/neha/Library/Python/3.9/lib/python/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: decorator in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.33->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.15-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (3.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/neha/Library/Python/3.9/lib/python/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/neha/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading langchain-0.3.17-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.61.1-py3-none-any.whl (463 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.2-cp39-cp39-macosx_11_0_arm64.whl (300 kB)\n",
      "Downloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch langchain openai ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (4.8.0)\n",
      "Collecting fastapi<1.0 (from gradio)\n",
      "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio)\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (0.28.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (3.1.5)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (3.9.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Using cached orjson-3.10.15-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (2.2.3)\n",
      "Collecting pillow<11.0,>=8.0 (from gradio)\n",
      "  Downloading pillow-10.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.9.4-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio) (2.3.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from gradio-client==1.3.0->gradio) (2024.9.0)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
      "  Downloading websockets-12.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi<1.0->gradio)\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: certifi in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->gradio) (3.17.0)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.3->gradio) (4.67.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.21.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.0->gradio)\n",
      "  Using cached pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.4.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/18.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 102, in read\n",
      "    self.__buf.write(data)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tempfile.py\", line 474, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py\", line 106, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py\", line 97, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/commands/install.py\", line 386, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_vendor/urllib3/response.py\", line 587, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/neha/Library/Python/3.9/lib/python/site-packages/pip/_vendor/urllib3/response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: OSError(28, 'No space left on device')\", OSError(28, 'No space left on device'))\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.16.2)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: keras in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.8.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.70.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.12.1)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp39-cp39-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (3.21.0)\n",
      "Using cached tensorflow-2.18.0-cp39-cp39-macosx_12_0_arm64.whl (239.4 MB)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-macos in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.16.2)\n",
      "Requirement already satisfied: keras in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.8.0)\n",
      "Requirement already satisfied: tensorflow==2.16.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.26.4)\n",
      "Requirement already satisfied: rich in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2->tensorflow-macos) (0.37.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow-macos keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5537b05f7f49d0bd703e04b8d5df92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1126\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py:34\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     TFBaseModelOutput,\n\u001b[1;32m     28\u001b[0m     TFMaskedLMOutput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     TFTokenClassifierOutput,\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[1;32m     36\u001b[0m     TFModelInputType,\n\u001b[1;32m     37\u001b[0m     TFMultipleChoiceLoss,\n\u001b[1;32m     38\u001b[0m     TFPreTrainedModel,\n\u001b[1;32m     39\u001b[0m     TFQuestionAnsweringLoss,\n\u001b[1;32m     40\u001b[0m     TFSequenceClassificationLoss,\n\u001b[1;32m     41\u001b[0m     TFTokenClassificationLoss,\n\u001b[1;32m     42\u001b[0m     get_initializer,\n\u001b[1;32m     43\u001b[0m     keras_serializable,\n\u001b[1;32m     44\u001b[0m     unpack_inputs,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m shape_list, stable_softmax\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_tf_utils.py:69\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend \u001b[39mas\u001b[39;00m K\n\u001b[0;32m---> 69\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m data_adapter\n\u001b[1;32m     70\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m KerasTensor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mUntitled-6.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m     intents \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=9'>10</a>\u001b[0m \u001b[39m# Load a pretrained Transformer model for intent classification\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=10'>11</a>\u001b[0m classifier \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mzero-shot-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdistilbert-base-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=12'>13</a>\u001b[0m \u001b[39m# Extract intent tags from JSON\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=13'>14</a>\u001b[0m intent_tags \u001b[39m=\u001b[39m [intent[\u001b[39m\"\u001b[39m\u001b[39mtag\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m intent \u001b[39min\u001b[39;00m intents[\u001b[39m\"\u001b[39m\u001b[39mintents\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/__init__.py:776\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    775\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 776\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    777\u001b[0m     model,\n\u001b[1;32m    778\u001b[0m     model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    779\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    780\u001b[0m     framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    781\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    782\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    783\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    784\u001b[0m )\n\u001b[1;32m    786\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    787\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:238\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m look_tf:\n\u001b[0;32m--> 238\u001b[0m     _class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(transformers_module, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTF\u001b[39;49m\u001b[39m{\u001b[39;49;00marchitecture\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m _class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1117\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m   1116\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1117\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[1;32m   1118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1116\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1115\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1116\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1117\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1118\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1128\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1129\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1130\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1131\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.engine'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Load a pretrained Transformer model for intent classification\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "# Extract intent tags from JSON\n",
    "intent_tags = [intent[\"tag\"] for intent in intents[\"intents\"]]\n",
    "\n",
    "# Function to classify intent using a transformer model\n",
    "def classify_intent(user_message):\n",
    "    result = classifier(user_message, intent_tags)\n",
    "    best_intent = result[\"labels\"][0]  # Highest confidence intent\n",
    "    return best_intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Using cached openai-1.61.1-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from openai) (4.8.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.8.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: sniffio in /Users/neha/Library/Python/3.9/lib/python/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Using cached openai-1.61.1-py3-none-any.whl (463 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.2-cp39-cp39-macosx_11_0_arm64.whl (300 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.8.2 openai-1.61.1 pydantic-2.10.6 pydantic-core-2.27.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.6.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('/usr/local/share/nltk_data/')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "Bot: Hi there, how can I help?\n",
      "Bot: Good to see you again!\n",
      "Bot: Good to see you again!\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern.lower())  # Convert to lowercase\n",
    "        responses[pattern.lower()] = random.choice(intent[\"responses\"])\n",
    "\n",
    "# Tokenize and preprocess the text\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to generate chatbot response\n",
    "def chatbot_response(user_input):\n",
    "    user_input = preprocess_text(user_input)\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "\n",
    "    if similarities[0, max_sim_index] < 0.2:  # Threshold for similarity\n",
    "        return \"I'm not sure how to respond to that.\"\n",
    "\n",
    "    return responses[patterns[max_sim_index]]\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "    print(\"Bot:\", chatbot_response(user_input))\n",
    "#working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "Bot: Good to see you again!\n",
      "Bot: Hi there, how can I help?\n",
      "Bot: Good to see you again!\n",
      "Bot: College is open 8am-5pm Monday-Saturday!\n",
      "Bot: College is open 8am-5pm Monday-Saturday!\n",
      "Bot: For Fee detail visit <a target=\"_blank\" href=\"LINK\"> here</a>\n",
      "Bot: I am a Chatbot.\n",
      "Bot: I am your helper\n",
      "Bot: I can answer to low-intermediate questions regarding college\n",
      "Bot: You can contact at: NUMBER\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern.lower())  # Convert to lowercase\n",
    "        responses[pattern.lower()] = random.choice(intent[\"responses\"])\n",
    "\n",
    "# Tokenize and preprocess the text\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to generate chatbot response\n",
    "def chatbot_response(user_input):\n",
    "    user_input = preprocess_text(user_input)\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "\n",
    "    # If similarity is too low, try fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.2:\n",
    "        best_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:  # Accept if similarity is above 60%\n",
    "            return responses[best_match]\n",
    "\n",
    "        return \"I'm not sure how to respond to that.\"\n",
    "\n",
    "    return responses[patterns[max_sim_index]]\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "    print(\"Bot:\", chatbot_response(user_input))\n",
    "#working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tk in /Users/neha/Library/Python/3.9/lib/python/site-packages (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "Bot: Good to see you again!\n",
      "Bot: College is open 8am-5pm Monday-Saturday!\n",
      "Bot: Hello!\n",
      "Bot: Our University has Excellent Infrastructure. Campus is clean. Good IT Labs With Good Speed of Internet connection\n",
      "Bot: There is one huge and spacious library.timings are 8am to 6pm and for more visit <a target=\"blank\" href=\"ADD LIBRARY DETAIL LINK\">here</a>\n",
      "Bot: College is open 8am-5pm Monday-Saturday!\n",
      "Bot: welcome, anything else i can assist you with?\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "tags = {}  # Store intent tags for memory\n",
    "context = {}  # Stores user-specific memory\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern.lower())  # Convert to lowercase\n",
    "        responses[pattern.lower()] = random.choice(intent[\"responses\"])\n",
    "        tags[pattern.lower()] = intent[\"tag\"]  # Store intent tags\n",
    "\n",
    "# Tokenize and preprocess the text\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to generate chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default\"):\n",
    "    user_input = preprocess_text(user_input)\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "\n",
    "    # If similarity is too low, try fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.2:\n",
    "        best_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:  # Accept if similarity is above 60%\n",
    "            intent_tag = tags.get(best_match, None)\n",
    "            return handle_memory(user_id, best_match, intent_tag)\n",
    "\n",
    "        return \"I'm not sure how to respond to that.\"\n",
    "\n",
    "    matched_pattern = patterns[max_sim_index]\n",
    "    intent_tag = tags[matched_pattern]\n",
    "\n",
    "    return handle_memory(user_id, matched_pattern, intent_tag)\n",
    "\n",
    "# Function to handle memory and context\n",
    "def handle_memory(user_id, matched_pattern, intent_tag):\n",
    "    if user_id not in context:\n",
    "        context[user_id] = {}\n",
    "\n",
    "    # Check if user is introducing themselves\n",
    "    if \"name\" in matched_pattern and \"my name is\" in matched_pattern:\n",
    "        user_name = matched_pattern.split(\"my name is\")[-1].strip()\n",
    "        context[user_id][\"name\"] = user_name\n",
    "        return f\"Nice to meet you, {user_name}!\"\n",
    "\n",
    "    # Retrieve previous name if available\n",
    "    user_name = context[user_id].get(\"name\", \"friend\")\n",
    "\n",
    "    # Handle previous topics\n",
    "    if intent_tag:\n",
    "        context[user_id][\"last_topic\"] = intent_tag\n",
    "\n",
    "    response = responses.get(matched_pattern, \"I'm not sure how to respond to that.\")\n",
    "\n",
    "    # Personalize response if user has provided their name\n",
    "    if user_name != \"friend\":\n",
    "        response = f\"{user_name}, {response}\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "    print(\"Bot:\", chatbot_response(user_input, user_id=\"user_1\"))  # Assign unique ID for memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Not, Not, Hi there, how can I help?\n",
      "Bot: Not, Not, Hi there, how can I help?\n",
      "Bot: Nice to meet you, jitesh! I'll remember that.\n",
      "Bot: jitesh, jitesh, I am your helper\n",
      "Bot: jitesh, jitesh, I am a Chatbot.\n",
      "Bot: jitesh, jitesh, I am a Chatbot.\n",
      "Bot: Goodbye! Your chat history is saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "tags = {}  \n",
    "users_memory = {}  \n",
    "\n",
    "# Load stored conversations if exists\n",
    "try:\n",
    "    with open(\"/Users/neha/Documents/chatbot/memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    pass  \n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern.lower())  \n",
    "        responses[pattern.lower()] = random.choice(intent[\"responses\"])\n",
    "        tags[pattern.lower()] = intent[\"tag\"]  \n",
    "\n",
    "# Tokenization and preprocessing\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to determine sentiment\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0.2:\n",
    "        return \"positive\"\n",
    "    elif sentiment < -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to generate chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default\"):\n",
    "    user_input = preprocess_text(user_input)\n",
    "    sentiment = analyze_sentiment(user_input)  \n",
    "\n",
    "    # Ensure user memory exists and is a dictionary\n",
    "    if user_id not in users_memory or not isinstance(users_memory[user_id], dict):\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": []}\n",
    "\n",
    "    # Save conversation history correctly\n",
    "    users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "\n",
    "    # Ensure input is passed as a list to vectorizer\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "\n",
    "    # Name extraction & recall\n",
    "    if \"my name is\" in user_input:\n",
    "        user_name = user_input.split(\"my name is\")[-1].strip()\n",
    "        users_memory[user_id][\"name\"] = user_name  # Store name\n",
    "        return f\"Nice to meet you, {user_name}! I'll remember that.\"\n",
    "\n",
    "    # If similarity is too low, try fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.2:\n",
    "        best_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:  \n",
    "            intent_tag = tags.get(best_match, None)\n",
    "            response = handle_memory(user_id, best_match, intent_tag)\n",
    "        else:\n",
    "            response = handle_unknown(user_input, user_id)\n",
    "    else:\n",
    "        matched_pattern = patterns[max_sim_index]\n",
    "        intent_tag = tags[matched_pattern]\n",
    "        response = handle_memory(user_id, matched_pattern, intent_tag)\n",
    "\n",
    "    # Personalize response with remembered name\n",
    "    user_name = users_memory[user_id].get(\"name\", None)\n",
    "    if user_name:\n",
    "        response = f\"{user_name}, {response}\"\n",
    "\n",
    "    # Adjust response based on sentiment\n",
    "    if sentiment == \"negative\":\n",
    "        response = f\"I see you're upset. {response}\"\n",
    "    elif sentiment == \"positive\":\n",
    "        response = f\"That's great to hear! {response}\"\n",
    "\n",
    "    users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")  \n",
    "\n",
    "    # Save memory\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Function to handle memory and context\n",
    "def handle_memory(user_id, matched_pattern, intent_tag):\n",
    "    if user_id not in users_memory:\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": []}\n",
    "\n",
    "    user_name = users_memory[user_id].get(\"name\", None)\n",
    "\n",
    "    if intent_tag:\n",
    "        users_memory[user_id][\"last_topic\"] = intent_tag\n",
    "\n",
    "    response = responses.get(matched_pattern, \"I'm not sure how to respond to that.\")\n",
    "    return f\"{user_name}, {response}\" if user_name else response\n",
    "\n",
    "# Function to handle unknown questions\n",
    "def handle_unknown(user_input, user_id):\n",
    "    return \"That's an interesting question! Could you clarify?\"\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Bot: Goodbye! Your chat history is saved.\")\n",
    "        break\n",
    "    print(\"Bot:\", chatbot_response(user_input, user_id=\"user_1\"))  \n",
    "#nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Not, Good to see you again!\n",
      "Bot: Not, Hello!\n",
      "Bot: Not, Hello!\n",
      "Bot: Not, Hello!\n",
      "Bot: Not, Goodbye!\n",
      "Bot: Not, I am your helper\n",
      "Bot: Not, There is one huge and spacious library.timings are 8am to 6pm and for more visit <a target=\"blank\" href=\"ADD LIBRARY DETAIL LINK\">here</a>\n",
      "Bot: Not, There is one huge and spacious library.timings are 8am to 6pm and for more visit <a target=\"blank\" href=\"ADD LIBRARY DETAIL LINK\">here</a>\n",
      "Bot: Goodbye! Your chat history is saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "tags = {}  \n",
    "users_memory = {}  \n",
    "\n",
    "# Load stored conversations if exists\n",
    "try:\n",
    "    with open(\"memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    pass  \n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern.lower())  \n",
    "        responses[pattern.lower()] = random.choice(intent[\"responses\"])\n",
    "        tags[pattern.lower()] = intent[\"tag\"]  \n",
    "\n",
    "# Tokenization and preprocessing\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to determine sentiment\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0.2:\n",
    "        return \"positive\"\n",
    "    elif sentiment < -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to generate chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default\"):\n",
    "    user_input = preprocess_text(user_input)\n",
    "    sentiment = analyze_sentiment(user_input)  \n",
    "\n",
    "    # Ensure user memory exists and is a dictionary\n",
    "    if user_id not in users_memory or not isinstance(users_memory[user_id], dict):\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": []}\n",
    "\n",
    "    # Save conversation history correctly\n",
    "    users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "\n",
    "    # Ensure input is passed as a list to vectorizer\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "\n",
    "    # Name extraction & recall\n",
    "    if \"my name is\" in user_input:\n",
    "        user_name = user_input.split(\"my name is\")[-1].strip()\n",
    "        users_memory[user_id][\"name\"] = user_name  # Store name\n",
    "        return f\"Nice to meet you, {user_name}! I'll remember that.\"\n",
    "\n",
    "    # If similarity is too low, try fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.2:\n",
    "        best_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:  \n",
    "            intent_tag = tags.get(best_match, None)\n",
    "            response = handle_memory(user_id, best_match, intent_tag)\n",
    "        else:\n",
    "            response = handle_unknown(user_input, user_id)\n",
    "    else:\n",
    "        matched_pattern = patterns[max_sim_index]\n",
    "        intent_tag = tags[matched_pattern]\n",
    "        response = handle_memory(user_id, matched_pattern, intent_tag)\n",
    "\n",
    "    # Personalize response with remembered name (Fix: Prevent name repetition)\n",
    "    user_name = users_memory[user_id].get(\"name\", None)\n",
    "    if user_name and user_name not in response:\n",
    "        response = f\"{user_name}, {response}\"\n",
    "\n",
    "    # Adjust response based on sentiment\n",
    "    if sentiment == \"negative\":\n",
    "        response = f\"I see you're upset. {response} 😞\"\n",
    "    elif sentiment == \"positive\":\n",
    "        response = f\"That's great to hear! {response} 😊\"\n",
    "\n",
    "    users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")  \n",
    "\n",
    "    # Save memory\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Function to handle memory and context\n",
    "def handle_memory(user_id, matched_pattern, intent_tag):\n",
    "    if user_id not in users_memory:\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": []}\n",
    "\n",
    "    user_name = users_memory[user_id].get(\"name\", None)\n",
    "\n",
    "    if intent_tag:\n",
    "        users_memory[user_id][\"last_topic\"] = intent_tag\n",
    "\n",
    "    response = responses.get(matched_pattern, \"I'm not sure how to respond to that.\")\n",
    "    return response\n",
    "\n",
    "# Function to handle unknown questions\n",
    "def handle_unknown(user_input, user_id):\n",
    "    return \"That's an interesting question! Could you clarify?\"\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Bot: Goodbye! Your chat history is saved.\")\n",
    "        break\n",
    "    print(\"Bot:\", chatbot_response(user_input, user_id=\"user_1\"))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "Bot: Hi there, how can I help?\n",
      "Bot: Hi there, how can I help?\n",
      "Bot: Nice to meet you, Jitesh! I'll remember that.\n",
      "Bot: For Fee detail visit <a target=\"_blank\" href=\"LINK\"> here</a> 😊\n",
      "Bot: Nice to meet you, Feeling! I'll remember that.\n",
      "Bot: Here's our recent chat history:\n",
      "Bot: Hi there, how can I help?\n",
      "You: hello\n",
      "Bot: Hi there, how can I help?\n",
      "You: im feeling great today\n",
      "Bot: For Fee detail visit <a target=\"_blank\" href=\"LINK\"> here</a> 😊\n",
      "Bot: Timetable provide direct to the students OR To know about syllabus visit <a target=\"_blank\" href=\"TIMETABLE LINK\"> here</a>\n",
      "Bot: welcome, anything else i can assist you with? 😊\n",
      "Bot: Nice to meet you, Very! I'll remember that.\n",
      "Bot: Hmm, I'm not sure how to respond to that. Could you clarify? 😊\n",
      "Bot: welcome, anything else i can assist you with? 😊\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: 'memory.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mUntitled-6.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=147'>148</a>\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYou: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=148'>149</a>\u001b[0m \u001b[39mif\u001b[39;00m user_input\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=149'>150</a>\u001b[0m     save_memory()\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=150'>151</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBot: Goodbye! Your chat history is saved.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=151'>152</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32mUntitled-6.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=137'>138</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_memory\u001b[39m():\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=138'>139</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Saves user conversation history and memory to a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=139'>140</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mmemory.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=140'>141</a>\u001b[0m         pickle\u001b[39m.\u001b[39mdump(users_memory, f)\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-6.ipynb?jupyter-notebook#X34sdW50aXRsZWQ%3D?line=141'>142</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMemory saved!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: 'memory.pkl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re  # For better name extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from textblob import TextBlob  # For sentiment analysis\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "intent_tags = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern.lower())  # Convert to lowercase\n",
    "        responses[pattern.lower()] = random.choice(intent[\"responses\"])\n",
    "        intent_tags[pattern.lower()] = intent[\"tag\"]\n",
    "\n",
    "# Tokenize and preprocess the text\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercases and lemmatizes input text.\"\"\"\n",
    "    tokens = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Load or Initialize Memory (Tracks User Data)\n",
    "try:\n",
    "    with open(\"memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    users_memory = {}\n",
    "\n",
    "# **Sentiment Analysis**\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment using TextBlob.\"\"\"\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0.2:\n",
    "        return \"positive\"\n",
    "    elif analysis.sentiment.polarity < -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# **Improved Name Extraction**\n",
    "def extract_name(user_input):\n",
    "    \"\"\"Extracts name if user introduces themselves, avoiding common words like 'is', 'am'.\"\"\"\n",
    "    name_match = re.search(r\"my name is (\\w+)|i am (\\w+)\", user_input, re.IGNORECASE)\n",
    "    if name_match:\n",
    "        name = name_match.group(1) or name_match.group(2)  # Get first non-null match\n",
    "        if name.lower() not in [\"is\", \"am\", \"name\"]:  # Avoid incorrect names\n",
    "            return name.capitalize()\n",
    "    return None\n",
    "\n",
    "# **Get Chat History**\n",
    "def get_chat_history(user_id):\n",
    "    \"\"\"Retrieves last 5 messages from chat history.\"\"\"\n",
    "    if user_id in users_memory and users_memory[user_id][\"conversation\"]:\n",
    "        history = \"\\n\".join(users_memory[user_id][\"conversation\"][-5:])\n",
    "        return f\"Here's our recent chat history:\\n{history}\"\n",
    "    return \"No previous chat history found.\"\n",
    "\n",
    "# Function to generate chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default_user\"):\n",
    "    user_input = preprocess_text(user_input)\n",
    "\n",
    "    # Initialize memory if user is new\n",
    "    if user_id not in users_memory:\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": [], \"last_topic\": None}\n",
    "\n",
    "    # Name Extraction\n",
    "    if any(word in user_input for word in [\"name\", \"i am\", \"my name is\"]):\n",
    "        name = extract_name(user_input)\n",
    "        if name:\n",
    "            users_memory[user_id][\"name\"] = name\n",
    "            return f\"Nice to meet you, {name}! I'll remember that.\"\n",
    "\n",
    "    # Handle Chat History Request\n",
    "    if \"history\" in user_input or \"previous chat\" in user_input:\n",
    "        return get_chat_history(user_id)\n",
    "\n",
    "    # Compute similarity\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "\n",
    "    # If similarity is too low, try fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.2:\n",
    "        best_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:  # Accept if similarity is above 60%\n",
    "            response = responses[best_match]\n",
    "        else:\n",
    "            response = \"Hmm, I'm not sure how to respond to that. Could you clarify?\"\n",
    "    else:\n",
    "        response = responses[patterns[max_sim_index]]\n",
    "\n",
    "    # Track last topic user asked about\n",
    "    users_memory[user_id][\"last_topic\"] = intent_tags.get(patterns[max_sim_index], None)\n",
    "\n",
    "    # Adjust response based on sentiment\n",
    "    sentiment = analyze_sentiment(user_input)\n",
    "    if sentiment == \"positive\":\n",
    "        response += \" 😊\"\n",
    "    elif sentiment == \"negative\":\n",
    "        response += \" 😔 Is there anything I can do to help?\"\n",
    "\n",
    "    # Personalization\n",
    "    if users_memory[user_id][\"name\"]:\n",
    "        response = response.replace(\"{name}\", users_memory[user_id][\"name\"])\n",
    "\n",
    "    # Save conversation history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "    users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "\n",
    "    return response\n",
    "\n",
    "# Save memory at exit\n",
    "def save_memory():\n",
    "    \"\"\"Saves user conversation history and memory to a file.\"\"\"\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "    print(\"Memory saved!\")\n",
    "\n",
    "# **Main Chat Loop**\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            save_memory()\n",
    "            print(\"Bot: Goodbye! Your chat history is saved.\")\n",
    "            break\n",
    "        print(\"Bot:\", chatbot_response(user_input, user_id=\"user_1\"))\n",
    "except KeyboardInterrupt:\n",
    "    save_memory()\n",
    "    print(\"\\nBot: Chat memory saved. Exiting...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "intent_tags = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern.lower())  \n",
    "        responses[pattern.lower()] = intent[\"responses\"]\n",
    "        intent_tags[pattern.lower()] = intent[\"tag\"]\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercases and lemmatizes input text.\"\"\"\n",
    "    tokens = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Convert to TF-IDF Matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load or Initialize Memory\n",
    "try:\n",
    "    with open(\"/Users/neha/Documents/chatbot/memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    users_memory = {}\n",
    "\n",
    "# Function to save memory\n",
    "def save_memory():\n",
    "    \"\"\"Saves user conversation history to a file.\"\"\"\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "    print(\"Memory saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def find_best_match(user_input):\n",
    "    \"\"\"Finds the most relevant intent based on user input.\"\"\"\n",
    "    user_vec = vectorizer.transform([preprocess_text(user_input)])\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "\n",
    "    # If similarity is low, try fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.2:\n",
    "        best_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:\n",
    "            return best_match\n",
    "        return None\n",
    "    return patterns[max_sim_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment using TextBlob.\"\"\"\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0.2:\n",
    "        return \"positive\"\n",
    "    elif analysis.sentiment.polarity < -0.2:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def extract_name(user_input):\n",
    "    \"\"\"Extracts a name if the user introduces themselves.\"\"\"\n",
    "    name_match = re.search(r\"my name is (\\w+)|i am (\\w+)\", user_input, re.IGNORECASE)\n",
    "    if name_match:\n",
    "        name = name_match.group(1) or name_match.group(2)\n",
    "        if name.lower() not in [\"is\", \"am\", \"name\"]:\n",
    "            return name.capitalize()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:33\u001b[0;36m\u001b[0m\n\u001b[0;31m    if user_id not in users_memory:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            save_memory()\n",
    "            print(\"Bot: Goodbye! Your chat history is saved.\")\n",
    "            break\n",
    "\n",
    "        # Extract name if mentioned\n",
    "        name = extract_name(user_input)\n",
    "        if name:\n",
    "            users_memory[\"user\"][\"name\"] = name\n",
    "            print(f\"Bot: Nice to meet you, {name}! I'll remember that.\")\n",
    "            continue\n",
    "\n",
    "        # Find best matching intent\n",
    "        best_match = find_best_match(user_input)\n",
    "        if best_match:\n",
    "            response = random.choice(responses[best_match])\n",
    "        else:\n",
    "            response = \"Hmm, I'm not sure how to respond to that. Could you clarify?\"\n",
    "\n",
    "        # Add sentiment\n",
    "        sentiment = analyze_sentiment(user_input)\n",
    "        if sentiment == \"positive\":\n",
    "            response += \" 😊\"\n",
    "        elif sentiment == \"negative\":\n",
    "            response += \" 😔 Is there anything I can do to help?\"\n",
    "\n",
    "        # Save to memory\n",
    "     # Ensure user exists in memory before saving\n",
    "       if user_id not in users_memory:\n",
    "         users_memory[user_id] = {\"name\": None, \"conversation\": [], \"last_topic\": None}  # Properly indented\n",
    "\n",
    "# Save conversation history\n",
    "users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    save_memory()\n",
    "    print(\"\\nBot: Chat memory saved. Exiting...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "Bot: Good to see you again!\n",
      "Bot: We are Proud to tell you that our college provides ragging free environment, and we have strict rules against ragging\n",
      "Bot: Goodbye! Your chat history is saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "tags = {}\n",
    "\n",
    "# Load stored user memory\n",
    "try:\n",
    "    with open(\"memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    users_memory = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        processed_pattern = pattern.lower()\n",
    "        patterns.append(processed_pattern)\n",
    "        responses[processed_pattern] = intent[\"responses\"]  # Store list of responses\n",
    "        tags[processed_pattern] = intent[\"tag\"]\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenizes and lemmatizes input text.\"\"\"\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to determine sentiment\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment using TextBlob.\"\"\"\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0.2:\n",
    "        return \"positive\"\n",
    "    elif sentiment < -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to extract name\n",
    "def extract_name(text):\n",
    "    \"\"\"Extracts the user's name from the input text.\"\"\"\n",
    "    match = re.search(r\"my name is (\\w+)|i am (\\w+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1) or match.group(2)\n",
    "    return None\n",
    "\n",
    "# Function to retrieve chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default\"):\n",
    "    \"\"\"Generates chatbot response, handling memory and sentiment.\"\"\"\n",
    "    user_input = preprocess_text(user_input)\n",
    "    sentiment = analyze_sentiment(user_input)\n",
    "\n",
    "    # Ensure user memory exists\n",
    "    if user_id not in users_memory:\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": [], \"last_topic\": None}\n",
    "\n",
    "    # Save user input in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "\n",
    "    # Check if user provides their name\n",
    "    user_name = extract_name(user_input)\n",
    "    if user_name:\n",
    "        users_memory[user_id][\"name\"] = user_name\n",
    "        return f\"Nice to meet you, {user_name}! I'll remember that.\"\n",
    "\n",
    "    # Ensure input is passed as a list to vectorizer\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "    best_match = patterns[max_sim_index]\n",
    "\n",
    "    # Handle low-confidence matches with fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.3:\n",
    "        fuzzy_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:\n",
    "            best_match = fuzzy_match\n",
    "        else:\n",
    "            return handle_unknown(user_input, user_id)\n",
    "\n",
    "    # Retrieve response\n",
    "    response_list = responses.get(best_match, [\"I'm not sure how to respond to that.\"])\n",
    "    response = random.choice(response_list)  # Randomized to avoid repetition\n",
    "\n",
    "    # Adjust response based on sentiment\n",
    "    if sentiment == \"negative\":\n",
    "        response = f\"I see you're upset. {response}\"\n",
    "    elif sentiment == \"positive\":\n",
    "        response = f\"That's great to hear! {response}\"\n",
    "\n",
    "    # Save chatbot response in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "\n",
    "    # Save memory\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Function to handle unknown questions\n",
    "def handle_unknown(user_input, user_id):\n",
    "    \"\"\"Handles when chatbot does not recognize the input.\"\"\"\n",
    "    return \"I'm not sure how to respond to that. Can you clarify?\"\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Bot: Goodbye! Your chat history is saved.\")\n",
    "        break\n",
    "    print(\"Bot:\", chatbot_response(user_input, user_id=\"user_1\"))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello!\n",
      "Bot: Hi there, how can I help?\n",
      "Bot: We are Proud to tell you that our college provides ragging free environment, and we have strict rules against ragging\n",
      "Bot: Our University has Excellent Infrastructure. Campus is clean. Good IT Labs With Good Speed of Internet connection\n",
      "Bot: We are Proud to tell you that our college provides ragging free environment, and we have strict rules against ragging\n",
      "Bot: ENTER YOUR OWN UNIVERSITY UNIFORM CIRCULER\n",
      "Bot: ENTER YOUR OWN UNIVERSITY UNIFORM CIRCULER\n",
      "Bot: I see you're upset. Our university encourages all-round development of students and hence provides sports facilities in the campus. For more details visit<a target=\"_blank\" href=/\"(LINK IF HAVE)\">here</a>\n",
      "Bot: Nice to meet you, sad! I'll remember that.\n",
      "Bot: I see you're upset. I am glad I helped you\n",
      "Bot: That's great to hear! I am glad I helped you\n",
      "Bot: I am glad I helped you\n",
      "Bot: Goodbye!\n",
      "Bot: Goodbye! Your chat history is saved.\n",
      "Bot: Our university offers Information Technology, computer Engineering, Mechanical engineering,Chemical engineering, Civil engineering and extc Engineering.\n",
      "Bot: Goodbye! Your chat history is saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm not sure how to respond to that. Can you clarify?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm not sure how to respond to that. Can you clarify?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm not sure how to respond to that. Can you clarify?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm not sure how to respond to that. Can you clarify?\n",
      "Bot: Goodbye! Your chat history is saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "tags = {}\n",
    "\n",
    "# Load stored user memory\n",
    "try:\n",
    "    with open(\"memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    users_memory = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        processed_pattern = pattern.lower()\n",
    "        patterns.append(processed_pattern)\n",
    "        responses[processed_pattern] = intent[\"responses\"]  # Store list of responses\n",
    "        tags[processed_pattern] = intent[\"tag\"]\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenizes and lemmatizes input text.\"\"\"\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to determine sentiment\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment using TextBlob.\"\"\"\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0.2:\n",
    "        return \"positive\"\n",
    "    elif sentiment < -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to extract name\n",
    "def extract_name(text):\n",
    "    \"\"\"Extracts the user's name from the input text.\"\"\"\n",
    "    match = re.search(r\"my name is (\\w+)|i am (\\w+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1) or match.group(2)\n",
    "    return None\n",
    "\n",
    "# Function to retrieve chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default\"):\n",
    "    \"\"\"Generates chatbot response, handling memory and sentiment.\"\"\"\n",
    "    user_input = preprocess_text(user_input)\n",
    "    sentiment = analyze_sentiment(user_input)\n",
    "\n",
    "    # Ensure user memory exists\n",
    "    if user_id not in users_memory:\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": [], \"last_topic\": None}\n",
    "\n",
    "    # Save user input in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "\n",
    "    # Handle chat history retrieval\n",
    "    if \"show my history\" in user_input or \"what did we talk about\" in user_input:\n",
    "        return get_chat_history(user_id)\n",
    "\n",
    "    # Check if user provides their name\n",
    "    user_name = extract_name(user_input)\n",
    "    if user_name:\n",
    "        users_memory[user_id][\"name\"] = user_name\n",
    "        return f\"Nice to meet you, {user_name}! I'll remember that.\"\n",
    "\n",
    "    # Ensure input is passed as a list to vectorizer\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "    best_match = patterns[max_sim_index]\n",
    "\n",
    "    # Handle low-confidence matches with fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.3:\n",
    "        fuzzy_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:\n",
    "            best_match = fuzzy_match\n",
    "        else:\n",
    "            return handle_unknown(user_input, user_id)\n",
    "\n",
    "    # Retrieve response\n",
    "    response_list = responses.get(best_match, [\"I'm not sure how to respond to that.\"])\n",
    "    response = random.choice(response_list)  # Randomized to avoid repetition\n",
    "\n",
    "    # Adjust response based on sentiment\n",
    "    if sentiment == \"negative\":\n",
    "        response = f\"I see you're upset. {response}\"\n",
    "    elif sentiment == \"positive\":\n",
    "        response = f\"That's great to hear! {response}\"\n",
    "\n",
    "    # Save chatbot response in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "\n",
    "    # Save memory\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Function to handle chat history retrieval\n",
    "def get_chat_history(user_id):\n",
    "    \"\"\"Retrieves last 5 messages from chat history.\"\"\"\n",
    "    if user_id in users_memory and users_memory[user_id][\"conversation\"]:\n",
    "        history = \"\\n\".join(users_memory[user_id][\"conversation\"][-5:])\n",
    "        return f\"Here's our recent chat history:\\n{history}\"\n",
    "    return \"No previous chat history found.\"\n",
    "\n",
    "# Function to handle unknown questions\n",
    "def handle_unknown(user_input, user_id):\n",
    "    \"\"\"Handles when chatbot does not recognize the input.\"\"\"\n",
    "    return \"I'm not sure how to respond to that. Can you clarify?\"\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_id = input(\"Enter your user ID: \")  # Allows multiple users\n",
    "    while True:\n",
    "        user_input = input(f\"You ({user_id}): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Bot: Goodbye! Your chat history is saved.\")\n",
    "            break\n",
    "        print(\"Bot:\", chatbot_response(user_input, user_id=user_id))\n",
    "#nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "\n",
      "Welcome ! Let's chat.\n",
      "Bot: Hello!\n",
      "Bot: Nice to meet you, jitesh! I'll remember that.\n",
      "Bot: Nice to meet you, jitesh! I'll remember that.\n",
      "Bot: Our University has Excellent Infrastructure. Campus is clean. Good IT Labs With Good Speed of Internet connection\n",
      "Bot: For Fee detail visit <a target=\"_blank\" href=\"LINK\"> here</a>\n",
      "Bot: There is one huge and spacious library.timings are 8am to 6pm and for more visit <a target=\"blank\" href=\"ADD LIBRARY DETAIL LINK\">here</a>\n",
      "Bot: There is one huge and spacious library.timings are 8am to 6pm and for more visit <a target=\"blank\" href=\"ADD LIBRARY DETAIL LINK\">here</a>\n",
      "Bot: Goodbye, ! Your chat history is saved.\n",
      "\n",
      "\n",
      "Welcome quit! Let's chat.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "tags = {}\n",
    "\n",
    "# Load stored user memory\n",
    "try:\n",
    "    with open(\"memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    users_memory = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        processed_pattern = pattern.lower()\n",
    "        patterns.append(processed_pattern)\n",
    "        responses[processed_pattern] = intent[\"responses\"]\n",
    "        tags[processed_pattern] = intent[\"tag\"]\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenizes and lemmatizes input text.\"\"\"\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to determine sentiment\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment using TextBlob.\"\"\"\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0.2:\n",
    "        return \"positive\"\n",
    "    elif sentiment < -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to extract name\n",
    "def extract_name(text):\n",
    "    \"\"\"Extracts the user's name from the input text.\"\"\"\n",
    "    match = re.search(r\"my name is (\\w+)|i am (\\w+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1) or match.group(2)\n",
    "    return None\n",
    "\n",
    "# Function to retrieve chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default\"):\n",
    "    \"\"\"Generates chatbot response, handling memory and sentiment.\"\"\"\n",
    "    user_input = preprocess_text(user_input)\n",
    "    sentiment = analyze_sentiment(user_input)\n",
    "\n",
    "    # Ensure user memory exists\n",
    "    if user_id not in users_memory:\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": []}\n",
    "\n",
    "    # Save user input in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "\n",
    "    # Handle chat history retrieval\n",
    "    if \"show my history\" in user_input or \"what did we talk about\" in user_input:\n",
    "        return get_chat_history(user_id)\n",
    "\n",
    "    # Check if user provides their name\n",
    "    user_name = extract_name(user_input)\n",
    "    if user_name:\n",
    "        users_memory[user_id][\"name\"] = user_name\n",
    "        response = f\"Nice to meet you, {user_name}! I'll remember that.\"\n",
    "        users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "        return response\n",
    "\n",
    "    # Ensure input is passed as a list to vectorizer\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "    best_match = patterns[max_sim_index]\n",
    "\n",
    "    # Handle low-confidence matches with fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.3:\n",
    "        fuzzy_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:\n",
    "            best_match = fuzzy_match\n",
    "        else:\n",
    "            response = handle_unknown(user_input, user_id)\n",
    "            users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "            return response\n",
    "\n",
    "    # Retrieve response\n",
    "    response_list = responses.get(best_match, [\"I'm not sure how to respond to that.\"])\n",
    "    response = random.choice(response_list)\n",
    "\n",
    "    # Adjust response based on sentiment\n",
    "    if sentiment == \"negative\":\n",
    "        response = f\"I see you're upset. {response}\"\n",
    "    elif sentiment == \"positive\":\n",
    "        response = f\"That's great to hear! {response}\"\n",
    "\n",
    "    # Save chatbot response in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "\n",
    "    # Save memory\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Function to handle chat history retrieval\n",
    "def get_chat_history(user_id):\n",
    "    \"\"\"Retrieves last 5 messages from chat history and formats them properly.\"\"\"\n",
    "    if user_id in users_memory and users_memory[user_id][\"conversation\"]:\n",
    "        history = \"\\n\".join(users_memory[user_id][\"conversation\"][-5:])\n",
    "        return f\"🕰️ Here's our recent chat history:\\n{history}\"\n",
    "    return \"No previous chat history found.\"\n",
    "\n",
    "# Function to handle unknown questions\n",
    "def handle_unknown(user_input, user_id):\n",
    "    \"\"\"Handles when chatbot does not recognize the input.\"\"\"\n",
    "    return \"I'm not sure how to respond to that. Can you clarify?\"\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_id = input(\"Enter your user ID: \")  # Allows multiple users\n",
    "    print(f\"\\nWelcome {user_id}! Let's chat.\")\n",
    "    while True:\n",
    "        user_input = input(f\"You ({user_id}): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(f\"Bot: Goodbye, {user_id}! Your chat history is saved.\\n\")\n",
    "            break\n",
    "        print(f\"Bot:\", chatbot_response(user_input, user_id=user_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "\n",
      "Welcome ! Let's chat.\n",
      "Bot: Good to see you again!\n",
      "Bot: Good to see you again!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "tags = {}\n",
    "\n",
    "# Load stored user memory\n",
    "try:\n",
    "    with open(\"memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    users_memory = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        processed_pattern = pattern.lower()\n",
    "        patterns.append(processed_pattern)\n",
    "        responses[processed_pattern] = intent[\"responses\"]\n",
    "        tags[processed_pattern] = intent[\"tag\"]\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenizes and lemmatizes input text.\"\"\"\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to determine sentiment\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment using TextBlob.\"\"\"\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0.2:\n",
    "        return \"positive\"\n",
    "    elif sentiment < -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to extract name\n",
    "def extract_name(text):\n",
    "    \"\"\"Extracts the user's name from the input text.\"\"\"\n",
    "    match = re.search(r\"my name is (\\w+)|i am (\\w+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1) or match.group(2)\n",
    "    return None\n",
    "\n",
    "# Function to retrieve chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default\"):\n",
    "    \"\"\"Generates chatbot response, handling memory and sentiment.\"\"\"\n",
    "    user_input = preprocess_text(user_input)\n",
    "    sentiment = analyze_sentiment(user_input)\n",
    "\n",
    "    # Ensure user memory exists\n",
    "    if user_id not in users_memory:\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": []}\n",
    "\n",
    "    # Save user input in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "\n",
    "    # Handle chat history retrieval\n",
    "    if \"show my history\" in user_input or \"what did we talk about\" in user_input:\n",
    "        return get_chat_history(user_id)\n",
    "\n",
    "    # Check if user provides their name\n",
    "    user_name = extract_name(user_input)\n",
    "    if user_name:\n",
    "        users_memory[user_id][\"name\"] = user_name\n",
    "        response = f\"Nice to meet you, {user_name}! I'll remember that.\"\n",
    "        users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "        return response\n",
    "\n",
    "    # Ensure input is passed as a list to vectorizer\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "    best_match = patterns[max_sim_index]\n",
    "\n",
    "    # Handle low-confidence matches with fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.3:\n",
    "        fuzzy_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:\n",
    "            best_match = fuzzy_match\n",
    "        else:\n",
    "            response = handle_unknown(user_input, user_id)\n",
    "            users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "            return response\n",
    "\n",
    "    # Retrieve response (Fix: Random choice only once)\n",
    "    response_list = responses.get(best_match, [\"I'm not sure how to respond to that.\"])\n",
    "    response = random.choice(response_list)\n",
    "\n",
    "    # Avoid repeating same response twice in a row\n",
    "    last_response = users_memory[user_id][\"conversation\"][-1] if users_memory[user_id][\"conversation\"] else None\n",
    "    if last_response == f\"Bot: {response}\":\n",
    "        response = \"Let's talk about something else! 😊\"\n",
    "\n",
    "    # Adjust response based on sentiment\n",
    "    if sentiment == \"negative\":\n",
    "        response = f\"I see you're upset. {response}\"\n",
    "    elif sentiment == \"positive\":\n",
    "        response = f\"That's great to hear! {response}\"\n",
    "\n",
    "    # Save chatbot response in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "\n",
    "    # Save memory\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Function to handle chat history retrieval\n",
    "def get_chat_history(user_id):\n",
    "    \"\"\"Retrieves last 5 messages from chat history and formats them properly.\"\"\"\n",
    "    if user_id in users_memory and users_memory[user_id][\"conversation\"]:\n",
    "        history = \"\\n\".join(users_memory[user_id][\"conversation\"][-5:])\n",
    "        return f\"🕰️ Here's our recent chat history:\\n{history}\"\n",
    "    return \"No previous chat history found.\"\n",
    "\n",
    "# Function to handle unknown questions\n",
    "def handle_unknown(user_input, user_id):\n",
    "    \"\"\"Handles when chatbot does not recognize the input.\"\"\"\n",
    "    return \"I'm not sure how to respond to that. Can you clarify?\"\n",
    "\n",
    "# Interactive Chat Loop\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_id = input(\"Enter your user ID: \")  # Allows multiple users\n",
    "    print(f\"\\nWelcome {user_id}! Let's chat.\")\n",
    "    while True:\n",
    "        user_input = input(f\"You ({user_id}): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(f\"Bot: Goodbye, {user_id}! Your chat history is saved.\\n\")\n",
    "            break\n",
    "        print(f\"Bot:\", chatbot_response(user_input, user_id=user_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "\n",
      "Welcome hi! Let's chat.\n",
      "Bot: Nice to meet you, jitesh! I'll remember that.\n",
      "Bot: That's great to hear! For Fee detail visit <a target=\"_blank\" href=\"LINK\"> here</a>\n",
      "Bot: Our university encourages all-round development of students and hence provides sports facilities in the campus. For more details visit<a target=\"_blank\" href=/\"(LINK IF HAVE)\">here</a>\n",
      "Bot: For event detail visit <a target=\"_blank\" href=\"ADD YOUR FUNCTIONS LINK OR YOUR OWN RESPONSE\"> here</a>\n",
      "Bot: To know more about document required visit <a target=\"_blank\" href=\"ADD LINK OF ADMISSION GUIDANCE DOCUMENT FROM YOUR UNIVERSITY WEBSITE\"> here</a>\n",
      "Bot: That's great to hear! My College has total 2 floors \n",
      "Bot: That's great to hear! welcome, anything else i can assist you with?\n",
      "Bot: Come back soon\n",
      "Bot: We are Proud to tell you that our college provides ragging free environment, and we have strict rules against ragging\n",
      "Bot: 🕰️ Here's our recent chat history:\n",
      "You: bye\n",
      "Bot: Come back soon\n",
      "You: history\n",
      "Bot: We are Proud to tell you that our college provides ragging free environment, and we have strict rules against ragging\n",
      "You: show my history\n",
      "Bot: Goodbye, hi! Your chat history is saved.\n",
      "\n",
      "\n",
      "Welcome quit! Let's chat.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load intents.json\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and responses\n",
    "patterns = []\n",
    "responses = {}\n",
    "tags = {}\n",
    "\n",
    "# Load stored user memory\n",
    "try:\n",
    "    with open(\"memory.pkl\", \"rb\") as f:\n",
    "        users_memory = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    users_memory = {}\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        processed_pattern = pattern.lower()\n",
    "        patterns.append(processed_pattern)\n",
    "        responses[processed_pattern] = intent[\"responses\"]\n",
    "        tags[processed_pattern] = intent[\"tag\"]\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenizes and lemmatizes input text.\"\"\"\n",
    "    tokens = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preprocess all patterns\n",
    "corpus = [preprocess_text(p) for p in patterns]\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Function to determine sentiment\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment using TextBlob.\"\"\"\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment > 0.2:\n",
    "        return \"positive\"\n",
    "    elif sentiment < -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to extract name\n",
    "def extract_name(text):\n",
    "    \"\"\"Extracts the user's name from the input text.\"\"\"\n",
    "    match = re.search(r\"my name is (\\w+)|i am (\\w+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1) or match.group(2)\n",
    "    return None\n",
    "\n",
    "# Function to retrieve chatbot response\n",
    "def chatbot_response(user_input, user_id=\"default\"):\n",
    "    \"\"\"Generates chatbot response, handling memory and sentiment.\"\"\"\n",
    "    user_input = preprocess_text(user_input)\n",
    "    sentiment = analyze_sentiment(user_input)\n",
    "\n",
    "    # Ensure user memory exists\n",
    "    if user_id not in users_memory:\n",
    "        users_memory[user_id] = {\"name\": None, \"conversation\": []}\n",
    "\n",
    "    # Save user input in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"You: {user_input}\")\n",
    "\n",
    "    # Handle chat history retrieval\n",
    "    if \"show my history\" in user_input or \"what did we talk about\" in user_input:\n",
    "        return get_chat_history(user_id)\n",
    "\n",
    "    # Check if user provides their name\n",
    "    user_name = extract_name(user_input)\n",
    "    if user_name:\n",
    "        users_memory[user_id][\"name\"] = user_name\n",
    "        response = f\"Nice to meet you, {user_name}! I'll remember that.\"\n",
    "        users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "        return response\n",
    "\n",
    "    # Ensure input is passed as a list to vectorizer\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(user_vec, X)\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "    best_match = patterns[max_sim_index]\n",
    "\n",
    "    # Handle low-confidence matches with fuzzy matching\n",
    "    if similarities[0, max_sim_index] < 0.3:\n",
    "        fuzzy_match, score = process.extractOne(user_input, patterns)\n",
    "        if score > 60:\n",
    "            best_match = fuzzy_match\n",
    "        else:\n",
    "            response = handle_unknown(user_input, user_id)\n",
    "            users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "            return response\n",
    "\n",
    "    # Retrieve response (Fix: Random choice only once)\n",
    "    response_list = responses.get(best_match, [\"I'm not sure how to respond to that.\"])\n",
    "    response = random.choice(response_list)\n",
    "\n",
    "    # **Fix:** Prevent duplicate responses\n",
    "    if len(users_memory[user_id][\"conversation\"]) > 1:\n",
    "        last_response = users_memory[user_id][\"conversation\"][-2]  # Get previous user input\n",
    "        if last_response == f\"Bot: {response}\":\n",
    "            response = \"Let's talk about something else! 😊\"\n",
    "\n",
    "    # Adjust response based on sentiment\n",
    "    if sentiment == \"negative\":\n",
    "        response = f\"I see you're upset. {response}\"\n",
    "    elif sentiment == \"positive\":\n",
    "        response = f\"That's great to hear! {response}\"\n",
    "\n",
    "    # Save chatbot response in chat history\n",
    "    users_memory[user_id][\"conversation\"].append(f\"Bot: {response}\")\n",
    "\n",
    "    # Save memory\n",
    "    with open(\"memory.pkl\", \"wb\") as f:\n",
    "        pickle.dump(users_memory, f)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Function to handle chat history retrieval\n",
    "def get_chat_history(user_id):\n",
    "    \"\"\"Retrieves last 5 messages from chat history and formats them properly.\"\"\"\n",
    "    if user_id in users_memory and users_memory[user_id][\"conversation\"]:\n",
    "        history = \"\\n\".join(users_memory[user_id][\"conversation\"][-5:])\n",
    "        return f\"🕰️ Here's our recent chat history:\\n{history}\"\n",
    "    return \"No previous chat history found.\"\n",
    "\n",
    "# Function to handle unknown questions\n",
    "def handle_unknown(user_input, user_id):\n",
    "    \"\"\"Handles when chatbot does not recognize the input.\"\"\"\n",
    "    return \"I'm not sure how to respond to that. Can you clarify?\"\n",
    "\n",
    "# **Interactive Chat Loop**\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    user_id = input(\"Enter your user ID: \")  # Allows multiple users\n",
    "    print(f\"\\nWelcome {user_id}! Let's chat.\")\n",
    "    while True:\n",
    "        user_input = input(f\"You ({user_id}): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(f\"Bot: Goodbye, {user_id}! Your chat history is saved.\\n\")\n",
    "            break\n",
    "        print(f\"Bot:\", chatbot_response(user_input, user_id=user_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
