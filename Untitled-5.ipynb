{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/neha/Library/Python/3.9/lib/python/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.3)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas matplotlib nltk scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              question  \\\n",
      "0               hi, how are you doing?   \n",
      "1        i'm fine. how about yourself?   \n",
      "2  i'm pretty good. thanks for asking.   \n",
      "3    no problem. so how have you been?   \n",
      "4     i've been great. what about you?   \n",
      "\n",
      "                                     answer  \n",
      "0             i'm fine. how about yourself?  \n",
      "1       i'm pretty good. thanks for asking.  \n",
      "2         no problem. so how have you been?  \n",
      "3          i've been great. what about you?  \n",
      "4  i've been good. i'm in school right now.  \n"
     ]
    }
   ],
   "source": [
    "columns = ['question', 'answer']\n",
    "df = pd.read_csv('/Users/neha/Documents/chatbot/dialogs.txt', sep='\\t', names=columns)\n",
    "\n",
    "# Display some examples\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)  # Extract words (ignores punctuation)\n",
    "    return tokens\n",
    "\n",
    "df['question_tokens'] = df['question'].apply(preprocess_text)\n",
    "df['answer_tokens'] = df['answer'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /usr/local/share/nltk_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2457\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Combine all words from questions and answers\n",
    "all_words = [word for tokens in df['question_tokens'] for word in tokens] + \\\n",
    "            [word for tokens in df['answer_tokens'] for word in tokens]\n",
    "\n",
    "word_freq = Counter(all_words)  # Count word frequencies\n",
    "vocab = sorted(word_freq, key=word_freq.get, reverse=True)  # Sort by frequency\n",
    "vocab_size = len(vocab) + 2  # +2 for <PAD> and <UNK>\n",
    "\n",
    "# Create mappings\n",
    "word2idx = {word: idx+2 for idx, word in enumerate(vocab)}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary Size: {len(word2idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          question_encoded  \\\n",
      "0    [1497, 39, 18, 3, 174, 0, 0, 0, 0, 0]   \n",
      "1    [2, 32, 602, 39, 35, 550, 0, 0, 0, 0]   \n",
      "2  [2, 32, 158, 46, 242, 27, 484, 0, 0, 0]   \n",
      "3    [31, 170, 23, 39, 16, 3, 99, 0, 0, 0]   \n",
      "4     [2, 69, 99, 103, 11, 35, 3, 0, 0, 0]   \n",
      "\n",
      "                            answer_encoded  \n",
      "0    [2, 32, 602, 39, 35, 550, 0, 0, 0, 0]  \n",
      "1  [2, 32, 158, 46, 242, 27, 484, 0, 0, 0]  \n",
      "2    [31, 170, 23, 39, 16, 3, 99, 0, 0, 0]  \n",
      "3     [2, 69, 99, 103, 11, 35, 3, 0, 0, 0]  \n",
      "4  [2, 69, 99, 46, 2, 32, 19, 93, 68, 112]  \n"
     ]
    }
   ],
   "source": [
    "def encode_sentence(tokens, word2idx, max_len=10):\n",
    "    encoded = [word2idx.get(word, word2idx['<UNK>']) for word in tokens]\n",
    "    return encoded[:max_len] + [word2idx['<PAD>']] * (max_len - len(encoded))\n",
    "\n",
    "df['question_encoded'] = df['question_tokens'].apply(lambda x: encode_sentence(x, word2idx))\n",
    "df['answer_encoded'] = df['answer_tokens'].apply(lambda x: encode_sentence(x, word2idx))\n",
    "\n",
    "print(df[['question_encoded', 'answer_encoded']].head())  # Check encoded values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          question_encoded  \\\n",
      "0    [1497, 39, 18, 3, 174, 0, 0, 0, 0, 0]   \n",
      "1    [2, 32, 602, 39, 35, 550, 0, 0, 0, 0]   \n",
      "2  [2, 32, 158, 46, 242, 27, 484, 0, 0, 0]   \n",
      "3    [31, 170, 23, 39, 16, 3, 99, 0, 0, 0]   \n",
      "4     [2, 69, 99, 103, 11, 35, 3, 0, 0, 0]   \n",
      "\n",
      "                            answer_encoded  \n",
      "0    [2, 32, 602, 39, 35, 550, 0, 0, 0, 0]  \n",
      "1  [2, 32, 158, 46, 242, 27, 484, 0, 0, 0]  \n",
      "2    [31, 170, 23, 39, 16, 3, 99, 0, 0, 0]  \n",
      "3     [2, 69, 99, 103, 11, 35, 3, 0, 0, 0]  \n",
      "4  [2, 69, 99, 46, 2, 32, 19, 93, 68, 112]  \n"
     ]
    }
   ],
   "source": [
    "# Convert words to indexes for LSTM input\n",
    "def encode_sentence(tokens, word2idx, max_len=10):\n",
    "    encoded = [word2idx.get(word, word2idx['<UNK>']) for word in tokens]\n",
    "    return encoded[:max_len] + [word2idx['<PAD>']] * (max_len - len(encoded))\n",
    "\n",
    "df['question_encoded'] = df['question_tokens'].apply(lambda x: encode_sentence(x, word2idx))\n",
    "df['answer_encoded'] = df['answer_tokens'].apply(lambda x: encode_sentence(x, word2idx))\n",
    "\n",
    "print(df[['question_encoded', 'answer_encoded']].head())  # Check encoded values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 2980\n",
      "Testing Data Size: 745\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.questions = torch.tensor(df['question_encoded'].tolist(), dtype=torch.long)\n",
    "        self.answers = torch.tensor(df['answer_encoded'].tolist(), dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.questions[idx], self.answers[idx]\n",
    "\n",
    "dataset = ChatDataset(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Training Data Size:\", len(train_dataset))\n",
    "print(\"Testing Data Size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatbotLSTM(\n",
      "  (embedding): Embedding(2457, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=2457, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ChatbotLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(ChatbotLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "# Define hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize model\n",
    "model = ChatbotLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])  # Ignore padding in loss calculation\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 6.5615\n",
      "Epoch [2/10], Loss: 5.9667\n",
      "Epoch [3/10], Loss: 5.8790\n",
      "Epoch [4/10], Loss: 5.8215\n",
      "Epoch [5/10], Loss: 5.7697\n",
      "Epoch [6/10], Loss: 5.7102\n",
      "Epoch [7/10], Loss: 5.6241\n",
      "Epoch [8/10], Loss: 5.5302\n",
      "Epoch [9/10], Loss: 5.4116\n",
      "Epoch [10/10], Loss: 5.2836\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for questions, answers in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(questions)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        # Shift output to match expected target shape\n",
    "        output = output[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "        answers = answers[:, 1:].contiguous().view(-1)  # Shift target answers\n",
    "        \n",
    "        loss = criterion(output, answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.3809\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for questions, answers in test_loader:\n",
    "            output = model(questions)\n",
    "            output = output[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "            answers = answers[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, answers)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss/len(test_loader):.4f}\")\n",
    "\n",
    "evaluate_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: s a i to to a a a the the\n"
     ]
    }
   ],
   "source": [
    "def predict_answer(question, model, word2idx, idx2word, max_len=10):\n",
    "    model.eval()\n",
    "    question_tokens = preprocess_text(question)\n",
    "    question_encoded = encode_sentence(question_tokens, word2idx, max_len)\n",
    "    question_tensor = torch.tensor(question_encoded, dtype=torch.long).unsqueeze(0)  # Add batch dim\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(question_tensor)\n",
    "    \n",
    "    predicted_ids = torch.argmax(output, dim=2).squeeze(0).tolist()\n",
    "    predicted_words = [idx2word[idx] for idx in predicted_ids if idx != word2idx['<PAD>']]\n",
    "    \n",
    "    return \" \".join(predicted_words)\n",
    "\n",
    "# Test the chatbot\n",
    "print(\"Chatbot:\", predict_answer(\"how are you?\", model, word2idx, idx2word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: tensorflow in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.16.2)\n",
      "Requirement already satisfied: keras in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.8.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas nltk tensorflow keras scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tk\n",
      "  Downloading tk-0.1.0-py3-none-any.whl.metadata (693 bytes)\n",
      "Collecting pickle-mixin\n",
      "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
      "Building wheels for collected packages: pickle-mixin\n",
      "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=6008 sha256=4f91fa46d29cba6825c881e9a448bbc95079117686ada8112d8b59680bdf91ab\n",
      "  Stored in directory: /Users/neha/Library/Caches/pip/wheels/58/c6/8b/061bd4edc8cea2b2235758c3e50473d08499236fbfdd21e6b3\n",
      "Successfully built pickle-mixin\n",
      "Installing collected packages: tk, pickle-mixin\n",
      "Successfully installed pickle-mixin-1.0.2 tk-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tk pickle-mixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"/Users/neha/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 405 documents\n",
      "Total 38 classes: ['admission', 'canteen', 'college intake', 'committee', 'computerhod', 'course', 'creator', 'document', 'event', 'extchod', 'facilities', 'fees', 'floors', 'goodbye', 'greeting', 'hod', 'hostel', 'hours', 'infrastructure', 'ithod', 'library', 'location', 'menu', 'name', 'number', 'placement', 'principal', 'ragging', 'random', 'salutaion', 'scholarship', 'sem', 'sports', 'swear', 'syllabus', 'task', 'uniform', 'vacation']\n",
      "Total 277 unique lemmatized words: ['(your', '??', '???', 'a', 'about', 'ac', 'active', 'activity', 'address', 'admision', 'admission', 'against', 'ai/ml', 'allotment', 'am', 'an', 'and', 'antiragging', 'any', 'anyone', 'are', 'as', 'asshole', 'at', 'attend', 'automobile', 'available', 'average', 'be', 'between', 'big', 'bitch', 'book', 'boy', 'branch', 'branches?', 'bring', 'building', 'by', 'bye', 'cafetaria', 'call', 'called', 'campus', 'can', 'canteen', 'capacity', 'case', 'casuals', 'ce', 'chatting', 'chemical', 'civil', 'code', 'college', 'college?', 'come', 'committe', 'committee', 'comp', 'company', 'computer', 'conducted', 'contact', 'course', 'courses?', 'create', 'created', 'creator', 'cya', 'date', 'day', 'designed', 'detail', 'developer', 'different', 'distance', 'do', 'document', 'doe', 'done', 'dress', 'dresscode', 'dumb', 'during', 'each', 'eat?', 'end', 'engineering', 'engineering?', 'event', 'events?', 'exam', 'extc', 'facility', 'far', 'fee', 'first', 'floor', 'food', 'for', 'fourth', 'from', 'fuck', 'fucker', 'function', 'game', 'get', 'girl', 'give', 'go', 'good', 'goodbye', 'got', 'gtg', 'guy', 'have', 'held', 'hell', 'hello', 'help', 'here', 'heyy', 'hi', 'history', 'hod', 'holiday', 'hostel', 'hour', 'how', 'i', 'idiot', 'in', 'incident', 'info', 'information', 'infrastructure', 'intake', 'is', 'it', 'job', 'junior', 'k', 'later', 'leaving', 'lecture', 'library', 'list', 'located', 'location', 'long', 'love', 'made', 'many', 'marry', 'max', 'maximum', 'me', 'mechanical', 'menu', 'more', 'much', 'my', 'name', 'name)', 'name?', 'need', 'needed', 'next', 'nice', 'no', 'no?', 'non-ac', 'number', 'number?', 'of', 'offer', 'offered', 'office', 'ok', 'okay', 'okie', 'okk', 'on', 'open', 'operation', 'organised', 'package', 'per', 'phone', 'placement', 'please', 'practice', 'principal', \"principal's\", 'process', 'provide', 'provided', 'ragging', 'reach', 'recruitment', 'required', 'room', 'saturday', 'schedule', 'scholarship', 'seat', 'second', 'see', 'sem', 'semester', 'servive', 'should', 'shut', 'size', 'something', 'sport', 'start', 'student', 'stundent', 'stupid', 'syllabus', 'take', 'taken', 'taking', 'talk', 'tall', 'technology', 'telephone', 'tell', 'thank', 'thanks', 'the', 'there', 'there?', 'these', 'thing', 'third', 'this', 'time', 'timetable', 'timing', 'to', 'ttyl', 'u', \"uni's\", 'uni?', 'uniform', 'univrsity(uni)', 'up', 'use', 'vacation', 'variety', 'visit', 'we', 'wear', 'well', 'what', \"what's\", 'whats', 'whatsup', 'whatv', 'when', 'where', 'wheres', 'which', 'who', 'whom', 'why', 'will', 'work', 'working', 'ya', 'year', 'you', 'you?', 'your']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load JSON file\n",
    "with open(\"/Users/neha/Documents/chatbot/intents.json\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "words = []     # Vocabulary\n",
    "classes = []   # Labels\n",
    "documents = [] # Pairs (pattern, intent)\n",
    "ignore_words = [\"?\", \"!\", \".\", \",\"]\n",
    "\n",
    "# Tokenization using `split()` instead of `nltk.word_tokenize()`\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        word_list = pattern.lower().split()  # Simple tokenization\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent[\"tag\"]))\n",
    "        \n",
    "        if intent[\"tag\"] not in classes:\n",
    "            classes.append(intent[\"tag\"])\n",
    "\n",
    "# Lemmatization & Cleaning\n",
    "words = [lemmatizer.lemmatize(w) for w in words if w not in ignore_words]\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "# Save words and classes\n",
    "pickle.dump(words, open(\"words.pkl\", \"wb\"))\n",
    "pickle.dump(classes, open(\"classes.pkl\", \"wb\"))\n",
    "\n",
    "print(f\"Total {len(documents)} documents\")\n",
    "print(f\"Total {len(classes)} classes: {classes}\")\n",
    "print(f\"Total {len(words)} unique lemmatized words: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created successfully.\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = [lemmatizer.lemmatize(w) for w in doc[0]]\n",
    "\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "train_x = np.array(list(training[:, 0]))  # Input (X)\n",
    "train_y = np.array(list(training[:, 1]))  # Output (Y)\n",
    "\n",
    "print(\"Training data created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.0639 - loss: 3.6194\n",
      "Epoch 2/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.1925 - loss: 3.2561\n",
      "Epoch 3/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2382 - loss: 2.8767\n",
      "Epoch 4/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3141 - loss: 2.5011\n",
      "Epoch 5/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3917 - loss: 2.1961\n",
      "Epoch 6/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5246 - loss: 1.8337\n",
      "Epoch 7/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5257 - loss: 1.5896\n",
      "Epoch 8/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6246 - loss: 1.4502\n",
      "Epoch 9/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6851 - loss: 1.1933\n",
      "Epoch 10/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7182 - loss: 0.9831\n",
      "Epoch 11/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7417 - loss: 0.9259\n",
      "Epoch 12/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7697 - loss: 0.7806\n",
      "Epoch 13/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8228 - loss: 0.7569\n",
      "Epoch 14/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8088 - loss: 0.6729\n",
      "Epoch 15/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8167 - loss: 0.6641\n",
      "Epoch 16/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9025 - loss: 0.4698\n",
      "Epoch 17/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8482 - loss: 0.5135\n",
      "Epoch 18/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8832 - loss: 0.4238\n",
      "Epoch 19/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8604 - loss: 0.4201\n",
      "Epoch 20/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9255 - loss: 0.3195\n",
      "Epoch 21/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9296 - loss: 0.2677\n",
      "Epoch 22/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8998 - loss: 0.3574\n",
      "Epoch 23/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8871 - loss: 0.3592\n",
      "Epoch 24/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9427 - loss: 0.3311\n",
      "Epoch 25/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9264 - loss: 0.3107\n",
      "Epoch 26/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9365 - loss: 0.2519\n",
      "Epoch 27/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9155 - loss: 0.3016\n",
      "Epoch 28/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9377 - loss: 0.2544\n",
      "Epoch 29/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9258 - loss: 0.2174\n",
      "Epoch 30/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9267 - loss: 0.2507\n",
      "Epoch 31/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9438 - loss: 0.1795\n",
      "Epoch 32/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9243 - loss: 0.2830\n",
      "Epoch 33/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9501 - loss: 0.2284\n",
      "Epoch 34/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9430 - loss: 0.2716\n",
      "Epoch 35/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9478 - loss: 0.2064\n",
      "Epoch 36/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9367 - loss: 0.1784\n",
      "Epoch 37/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9429 - loss: 0.2028\n",
      "Epoch 38/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9361 - loss: 0.1782\n",
      "Epoch 39/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9543 - loss: 0.1695\n",
      "Epoch 40/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9741 - loss: 0.1170\n",
      "Epoch 41/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9351 - loss: 0.2044\n",
      "Epoch 42/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9229 - loss: 0.2615\n",
      "Epoch 43/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9289 - loss: 0.2605\n",
      "Epoch 44/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9423 - loss: 0.1965\n",
      "Epoch 45/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9711 - loss: 0.1348\n",
      "Epoch 46/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9633 - loss: 0.1139\n",
      "Epoch 47/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9575 - loss: 0.1686\n",
      "Epoch 48/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9673 - loss: 0.1225\n",
      "Epoch 49/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9701 - loss: 0.1205\n",
      "Epoch 50/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9655 - loss: 0.1556\n",
      "Epoch 51/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9779 - loss: 0.1416\n",
      "Epoch 52/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9562 - loss: 0.1373\n",
      "Epoch 53/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9482 - loss: 0.2494\n",
      "Epoch 54/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9627 - loss: 0.1485\n",
      "Epoch 55/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9683 - loss: 0.1013\n",
      "Epoch 56/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9581 - loss: 0.1555\n",
      "Epoch 57/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9576 - loss: 0.1776\n",
      "Epoch 58/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9726 - loss: 0.0967\n",
      "Epoch 59/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9773 - loss: 0.1137\n",
      "Epoch 60/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9538 - loss: 0.1507\n",
      "Epoch 61/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9777 - loss: 0.1296\n",
      "Epoch 62/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9718 - loss: 0.1128\n",
      "Epoch 63/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9513 - loss: 0.1873\n",
      "Epoch 64/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9493 - loss: 0.1418\n",
      "Epoch 65/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9641 - loss: 0.0986\n",
      "Epoch 66/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9742 - loss: 0.1219\n",
      "Epoch 67/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9659 - loss: 0.1081\n",
      "Epoch 68/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9791 - loss: 0.0972\n",
      "Epoch 69/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9858 - loss: 0.0637\n",
      "Epoch 70/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9819 - loss: 0.0922\n",
      "Epoch 71/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9745 - loss: 0.1808\n",
      "Epoch 72/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9463 - loss: 0.1725\n",
      "Epoch 73/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9638 - loss: 0.1499\n",
      "Epoch 74/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9759 - loss: 0.0834\n",
      "Epoch 75/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9883 - loss: 0.0948\n",
      "Epoch 76/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9683 - loss: 0.1486\n",
      "Epoch 77/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9671 - loss: 0.1696\n",
      "Epoch 78/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9690 - loss: 0.1259\n",
      "Epoch 79/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9640 - loss: 0.1213\n",
      "Epoch 80/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9619 - loss: 0.1294\n",
      "Epoch 81/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9776 - loss: 0.0729\n",
      "Epoch 82/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9256 - loss: 0.2441\n",
      "Epoch 83/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9494 - loss: 0.1242\n",
      "Epoch 84/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9850 - loss: 0.0606\n",
      "Epoch 85/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9710 - loss: 0.0857\n",
      "Epoch 86/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9625 - loss: 0.1761\n",
      "Epoch 87/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9663 - loss: 0.1275\n",
      "Epoch 88/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9663 - loss: 0.1361\n",
      "Epoch 89/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9677 - loss: 0.1382\n",
      "Epoch 90/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9670 - loss: 0.0822\n",
      "Epoch 91/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9406 - loss: 0.1177\n",
      "Epoch 92/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9843 - loss: 0.0505\n",
      "Epoch 93/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9604 - loss: 0.1500\n",
      "Epoch 94/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9651 - loss: 0.1097\n",
      "Epoch 95/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9637 - loss: 0.0870\n",
      "Epoch 96/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9773 - loss: 0.0678\n",
      "Epoch 97/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9748 - loss: 0.0667\n",
      "Epoch 98/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9715 - loss: 0.1210\n",
      "Epoch 99/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9697 - loss: 0.0965\n",
      "Epoch 100/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9751 - loss: 0.0710\n",
      "Epoch 101/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9700 - loss: 0.0861\n",
      "Epoch 102/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9800 - loss: 0.0820\n",
      "Epoch 103/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9663 - loss: 0.0959\n",
      "Epoch 104/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9873 - loss: 0.0673\n",
      "Epoch 105/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9841 - loss: 0.0869\n",
      "Epoch 106/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9675 - loss: 0.0952\n",
      "Epoch 107/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9774 - loss: 0.0922\n",
      "Epoch 108/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9687 - loss: 0.1034\n",
      "Epoch 109/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9553 - loss: 0.1558\n",
      "Epoch 110/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9747 - loss: 0.0736\n",
      "Epoch 111/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9557 - loss: 0.1885\n",
      "Epoch 112/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9763 - loss: 0.1136\n",
      "Epoch 113/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9728 - loss: 0.0895\n",
      "Epoch 114/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9576 - loss: 0.1157\n",
      "Epoch 115/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9678 - loss: 0.0792\n",
      "Epoch 116/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9521 - loss: 0.1911\n",
      "Epoch 117/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9638 - loss: 0.1297\n",
      "Epoch 118/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9612 - loss: 0.1640\n",
      "Epoch 119/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9493 - loss: 0.2022\n",
      "Epoch 120/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9719 - loss: 0.1140\n",
      "Epoch 121/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9395 - loss: 0.2504\n",
      "Epoch 122/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9631 - loss: 0.1846\n",
      "Epoch 123/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9517 - loss: 0.1443\n",
      "Epoch 124/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9342 - loss: 0.2368\n",
      "Epoch 125/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9445 - loss: 0.2294\n",
      "Epoch 126/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9428 - loss: 0.1313\n",
      "Epoch 127/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9638 - loss: 0.1602\n",
      "Epoch 128/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9652 - loss: 0.0891\n",
      "Epoch 129/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9482 - loss: 0.2115\n",
      "Epoch 130/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9490 - loss: 0.1643\n",
      "Epoch 131/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9693 - loss: 0.1058\n",
      "Epoch 132/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9497 - loss: 0.3072\n",
      "Epoch 133/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9448 - loss: 0.1779\n",
      "Epoch 134/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9754 - loss: 0.1172\n",
      "Epoch 135/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9808 - loss: 0.1167\n",
      "Epoch 136/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9715 - loss: 0.1877\n",
      "Epoch 137/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9571 - loss: 0.1875\n",
      "Epoch 138/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9385 - loss: 0.2371\n",
      "Epoch 139/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9578 - loss: 0.1527\n",
      "Epoch 140/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9533 - loss: 0.2863\n",
      "Epoch 141/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9214 - loss: 0.3844\n",
      "Epoch 142/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9706 - loss: 0.1483\n",
      "Epoch 143/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9681 - loss: 0.1520\n",
      "Epoch 144/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9569 - loss: 0.2175\n",
      "Epoch 145/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9634 - loss: 0.1485\n",
      "Epoch 146/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9638 - loss: 0.4033\n",
      "Epoch 147/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9713 - loss: 0.3035\n",
      "Epoch 148/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9568 - loss: 0.1582\n",
      "Epoch 149/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9592 - loss: 0.1736\n",
      "Epoch 150/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9365 - loss: 0.4037\n",
      "Epoch 151/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9248 - loss: 0.3528\n",
      "Epoch 152/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9744 - loss: 0.0768\n",
      "Epoch 153/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9541 - loss: 0.2464\n",
      "Epoch 154/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9546 - loss: 0.1506\n",
      "Epoch 155/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9605 - loss: 0.2624\n",
      "Epoch 156/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9470 - loss: 0.2240\n",
      "Epoch 157/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9472 - loss: 0.2565\n",
      "Epoch 158/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9602 - loss: 0.1411\n",
      "Epoch 159/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9479 - loss: 0.3693\n",
      "Epoch 160/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9569 - loss: 0.2110\n",
      "Epoch 161/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9627 - loss: 0.1892\n",
      "Epoch 162/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9631 - loss: 0.2119\n",
      "Epoch 163/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9473 - loss: 0.2888\n",
      "Epoch 164/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9404 - loss: 0.2384\n",
      "Epoch 165/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9182 - loss: 0.4112\n",
      "Epoch 166/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9542 - loss: 0.1961\n",
      "Epoch 167/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9456 - loss: 0.3251\n",
      "Epoch 168/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9084 - loss: 0.6165\n",
      "Epoch 169/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9402 - loss: 0.3092\n",
      "Epoch 170/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9229 - loss: 0.4328\n",
      "Epoch 171/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8910 - loss: 0.6807\n",
      "Epoch 172/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8765 - loss: 0.9267\n",
      "Epoch 173/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8796 - loss: 0.9591\n",
      "Epoch 174/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8564 - loss: 0.7750\n",
      "Epoch 175/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8788 - loss: 0.9078\n",
      "Epoch 176/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8284 - loss: 1.4705\n",
      "Epoch 177/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8025 - loss: 1.9107\n",
      "Epoch 178/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8327 - loss: 1.9083\n",
      "Epoch 179/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8145 - loss: 2.2779\n",
      "Epoch 180/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8380 - loss: 1.7350\n",
      "Epoch 181/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7727 - loss: 2.9410\n",
      "Epoch 182/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6517 - loss: 5.7238\n",
      "Epoch 183/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6806 - loss: 6.9434\n",
      "Epoch 184/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6700 - loss: 12.3307\n",
      "Epoch 185/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6145 - loss: 17.0403\n",
      "Epoch 186/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5303 - loss: 38.5902\n",
      "Epoch 187/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2752 - loss: 497.2097\n",
      "Epoch 188/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0400 - loss: nan\n",
      "Epoch 189/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 190/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 191/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 192/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 193/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 194/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 195/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 196/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 197/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 198/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 199/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 200/200\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation=\"softmax\"))\n",
    "\n",
    "# Compile model\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "# Save model\n",
    "model.save(\"chatbot_model.h5\")\n",
    "print(\"Model trained and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"/Users/neha/Documents/chatbot/chatbot_model.h5\")\n",
    "words = pickle.load(open(\"/Users/neha/Documents/chatbot/words.pkl\", \"rb\"))\n",
    "classes = pickle.load(open(\"/Users/neha/Documents/chatbot/classes.pkl\", \"rb\"))\n",
    "\n",
    "# Preprocess user input\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = sentence.lower().split()  # Tokenization using split()\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)\n",
    "\n",
    "def predict_class(sentence):\n",
    "    p = bow(sentence, words)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(msg):\n",
    "    intents_list = predict_class(msg)\n",
    "    intent = intents_list[0][\"intent\"] if intents_list else \"fallback\"\n",
    "\n",
    "    for i in intents[\"intents\"]:\n",
    "        if i[\"tag\"] == intent:\n",
    "            return random.choice(i[\"responses\"])\n",
    "\n",
    "    return \"I don't understand that.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "numpy() is only available when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mUntitled-5.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-5.ipynb?jupyter-notebook#X44sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_x, train_y, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-5.ipynb?jupyter-notebook#X44sdW50aXRsZWQ%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m/Users/neha/Documents/chatbot/chatbot_model.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/core.py:155\u001b[0m, in \u001b[0;36mconvert_to_numpy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, tf\u001b[39m.\u001b[39mRaggedTensor):\n\u001b[1;32m    154\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto_tensor()\n\u001b[0;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray(x)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: numpy() is only available when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\n",
    "model.save(\"/Users/neha/Documents/chatbot/chatbot_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88785860f1d04900b94cd6ab2b58429f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Chat Log:', disabled=True, layout=Layout(height='250px', width='95%'), placeho…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4f7c646ef546cc994774938fb02ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Your Message:', layout=Layout(width='80%'), placeholder='Enter your…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Function to get chatbot response\n",
    "def chatbot_response(msg):\n",
    "    intents_list = predict_class(msg)\n",
    "    intent = intents_list[0][\"intent\"] if intents_list else \"fallback\"\n",
    "\n",
    "    for i in intents[\"intents\"]:\n",
    "        if i[\"tag\"] == intent:\n",
    "            return random.choice(i[\"responses\"])\n",
    "\n",
    "    return \"I don't understand that.\"\n",
    "\n",
    "\n",
    "# Create a chat log area\n",
    "chat_log = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Chat History',\n",
    "    description='Chat Log:',\n",
    "    disabled=True,  # Disable editing for chat log\n",
    "    layout=widgets.Layout(height='250px', width='95%')\n",
    ")\n",
    "\n",
    "# Create an entry box for user input\n",
    "entry_box = widgets.Text(\n",
    "    placeholder='Enter your message...',\n",
    "    description='Your Message:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Create a send button\n",
    "send_button = widgets.Button(description=\"Send\", button_style='primary')\n",
    "\n",
    "# Function to handle user input and update chat log\n",
    "def on_button_clicked(b):\n",
    "    user_message = entry_box.value.strip()\n",
    "    entry_box.value = ''  # Clear the input field\n",
    "\n",
    "    if user_message:\n",
    "        chat_log.value += f\"You: {user_message}\\n\"\n",
    "        bot_response = chatbot_response(user_message)  # Get chatbot response\n",
    "        chat_log.value += f\"Bot: {bot_response}\\n\"\n",
    "\n",
    "# Bind button click to function\n",
    "send_button.on_click(on_button_clicked)\n",
    "\n",
    "# Display UI components in Jupyter Notebook\n",
    "display(chat_log)\n",
    "display(widgets.HBox([entry_box, send_button]))  # Place entry box and button side by side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "\n",
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\", \"end-1c\").strip()\n",
    "    EntryBox.delete(\"0.0\", END)\n",
    "\n",
    "    if msg != \"\":\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + \"\\n\\n\")\n",
    "        ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12))\n",
    "\n",
    "        res = chatbot_response(msg)\n",
    "        ChatLog.insert(END, \"Bot: \" + res + \"\\n\\n\")\n",
    "\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "\n",
    "# Create main window\n",
    "base = Tk()\n",
    "base.title(\"Chatbot\")\n",
    "base.geometry(\"400x500\")\n",
    "base.resizable(width=False, height=False)\n",
    "\n",
    "# Chat window\n",
    "ChatLog = Text(base, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\", state=DISABLED)\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview)\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "# Entry box\n",
    "EntryBox = Text(base, bd=0, bg=\"white\", width=\"29\", height=\"5\", font=\"Arial\")\n",
    "\n",
    "# Send button\n",
    "SendButton = Button(base, text=\"Send\", font=(\"Verdana\", 12, 'bold'), width=\"12\", height=\"5\", bg=\"#32de97\", command=send)\n",
    "\n",
    "# Place components\n",
    "scrollbar.place(x=376, y=6, height=386)\n",
    "ChatLog.place(x=6, y=6, height=386, width=370)\n",
    "EntryBox.place(x=128, y=401, height=90, width=265)\n",
    "SendButton.place(x=6, y=401, height=90)\n",
    "\n",
    "base.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "WINDOW_WIDTH = 400\n",
    "WINDOW_HEIGHT = 500\n",
    "\n",
    "def chatbot_response(message):\n",
    "    # Basic example:\n",
    "    if \"hello\" in message.lower():\n",
    "        return \"Hello there!\"\n",
    "    elif \"how are you\" in message.lower():\n",
    "        return \"I'm doing well, thank you.\"\n",
    "    elif \"bye\" in message.lower():\n",
    "        return \"Goodbye!\"\n",
    "    else:\n",
    "        return \"I didn't understand that.  Try saying hello, or asking how I am.\"\n",
    "\n",
    "def send_message():\n",
    "    user_message = entry_box.get(\"1.0\", tk.END).strip()\n",
    "    entry_box.delete(\"1.0\", tk.END)\n",
    "\n",
    "    if user_message:\n",
    "        chat_log.config(state=tk.NORMAL)\n",
    "        chat_log.insert(tk.END, f\"You: {user_message}\\n\\n\")\n",
    "        bot_response = chatbot_response(user_message)\n",
    "        chat_log.insert(tk.END, f\"Bot: {bot_response}\\n\\n\")\n",
    "        chat_log.config(state=tk.DISABLED)\n",
    "        chat_log.see(tk.END)  # Scroll to the bottom\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot\")\n",
    "root.geometry(f\"{WINDOW_WIDTH}x{WINDOW_HEIGHT}\")\n",
    "root.resizable(width=False, height=False)\n",
    "\n",
    "chat_log = scrolledtext.ScrolledText(root, wrap=tk.WORD, bg=\"white\")\n",
    "chat_log.config(state=tk.DISABLED)\n",
    "chat_log.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "entry_box = tk.Text(root, height=4, bg=\"white\")\n",
    "entry_box.pack(fill=tk.X)\n",
    "\n",
    "send_button = tk.Button(root, text=\"Send\", command=send_message)\n",
    "send_button.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from nltk.stem import WordNetLemmatizer  # Import WordNetLemmatizer\n",
    "\n",
    "# Load necessary files (make sure these paths are correct)\n",
    "try:\n",
    "    model = load_model(\"/Users/neha/Documents/chatbot/chatbot_model.h5\")\n",
    "    words = pickle.load(open(\"/Users/neha/Documents/chatbot/words.pkl\", \"rb\"))\n",
    "    classes = pickle.load(open(\"/Users/neha/Documents/chatbot/classes.pkl\", \"rb\"))\n",
    "    with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as f:\n",
    "        intents = json.load(f)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: One or more files not found: {e}\")\n",
    "    exit()  # Exit if files are missing\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = sentence.lower().split()\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)\n",
    "\n",
    "def predict_class(sentence):\n",
    "    try:\n",
    "        p = bow(sentence, words)\n",
    "        res = model.predict(np.array([p]))[0]\n",
    "        ERROR_THRESHOLD = 0.25\n",
    "        results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return_list = []\n",
    "        for r in results:\n",
    "            return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "        return return_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {e}\")\n",
    "        return []  # Return empty list in case of error\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    try:\n",
    "        intents_list = predict_class(msg)\n",
    "\n",
    "        if intents_list:  # Check if intents_list is not empty\n",
    "            intent = intents_list[0][\"intent\"]\n",
    "            for i in intents[\"intents\"]:\n",
    "                if i[\"tag\"] == intent:\n",
    "                    return random.choice(i[\"responses\"])\n",
    "        return \"I don't understand that.\"  # Default response if no intent is found or error occurs\n",
    "    except Exception as e:\n",
    "        print(f\"Error in response generation: {e}\")\n",
    "        return \"I encountered an error.\"\n",
    "\n",
    "\n",
    "\n",
    "# Tkinter GUI code (modified for better error handling and clarity)\n",
    "def send():\n",
    "    msg = entry_box.get(\"1.0\", tk.END).strip()\n",
    "    entry_box.delete(\"1.0\", tk.END)\n",
    "\n",
    "    if msg:  # Check if the message is not empty\n",
    "        chat_log.config(state=tk.NORMAL)\n",
    "        chat_log.insert(tk.END, f\"You: {msg}\\n\\n\")\n",
    "\n",
    "        try:\n",
    "            bot_response = chatbot_response(msg)\n",
    "            chat_log.insert(tk.END, f\"Bot: {bot_response}\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in send function: {e}\")\n",
    "            chat_log.insert(tk.END, \"Bot: I encountered an error.\\n\\n\")\n",
    "\n",
    "        chat_log.config(state=tk.DISABLED)\n",
    "        chat_log.see(tk.END)\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot\")\n",
    "\n",
    "chat_log = scrolledtext.ScrolledText(root, wrap=tk.WORD, bg=\"white\")\n",
    "chat_log.config(state=tk.DISABLED)\n",
    "chat_log.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "entry_box = tk.Text(root, height=4, bg=\"white\")\n",
    "entry_box.pack(fill=tk.X)\n",
    "\n",
    "send_button = tk.Button(root, text=\"Send\", command=send)\n",
    "send_button.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574355cd0b6a4c5c9be598bcdba6a4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Chat Log:', layout=Layout(height='250px', width='95%'), placeholder='Chat Hist…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe717fe3a3214c9589c23c23de514eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Your Message:', layout=Layout(width='80%'), placeholder='Enter your message...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d3787b55d348c1b34bcc031f678445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Send', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import random  # Make sure random is imported\n",
    "\n",
    "# Load necessary files\n",
    "try:\n",
    "    model = load_model(\"/Users/neha/Documents/chatbot/chatbot_model.h5\")\n",
    "    words = pickle.load(open(\"/Users/neha/Documents/chatbot/words.pkl\", \"rb\"))\n",
    "    classes = pickle.load(open(\"/Users/neha/Documents/chatbot/classes.pkl\", \"rb\"))\n",
    "    with open(\"/Users/neha/Documents/chatbot/intents.json\", \"r\") as f:\n",
    "        intents = json.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Model or data files not found: {e}\")\n",
    "    exit()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ***CRITICAL: Compile the loaded model (use the same parameters as during training)***\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Preprocessing functions\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = sentence.lower().split()\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)\n",
    "\n",
    "def predict_class(sentence):\n",
    "    p = bow(sentence, words)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "# Chatbot response function\n",
    "def chatbot_response(message):\n",
    "    intents_list = predict_class(message)\n",
    "    if intents_list:\n",
    "        intent = intents_list[0][\"intent\"]\n",
    "        for i in intents[\"intents\"]:\n",
    "            if i[\"tag\"] == intent:\n",
    "                return random.choice(i[\"responses\"])  # Use random.choice here\n",
    "    return \"I don't understand that.\"\n",
    "\n",
    "# Interactive widgets\n",
    "chat_log = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Chat History',\n",
    "    description='Chat Log:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(height='250px', width='95%')\n",
    ")\n",
    "\n",
    "entry_box = widgets.Text(\n",
    "    placeholder='Enter your message...',\n",
    "    description='Your Message:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "send_button = widgets.Button(description=\"Send\", button_style='primary')\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    user_message = entry_box.value.strip()\n",
    "    entry_box.value = ''\n",
    "\n",
    "    if user_message:\n",
    "        chat_log.value += f\"You: {user_message}\\n\"\n",
    "        bot_response = chatbot_response(user_message)\n",
    "        chat_log.value += f\"Bot: {bot_response}\\n\"\n",
    "\n",
    "send_button.on_click(on_button_clicked)\n",
    "\n",
    "display(chat_log)\n",
    "display(entry_box)\n",
    "display(send_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
