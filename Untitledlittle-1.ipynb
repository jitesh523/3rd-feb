{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.16.2)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: rich in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow pandas numpy scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (286472854.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    wget http://nlp.stanford.edu/data/glove.6B.zip\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4082407053.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python train_chatbot.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python train_chatbot.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GloVe embeddings loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.0162 - loss: 8.0928 - val_accuracy: 0.0000e+00 - val_loss: 8.1827\n",
      "Epoch 2/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0163 - loss: 8.0497 - val_accuracy: 0.0268 - val_loss: 8.5056\n",
      "Epoch 3/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0297 - loss: 7.8017 - val_accuracy: 0.0268 - val_loss: 9.1084\n",
      "Epoch 4/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0324 - loss: 7.6005 - val_accuracy: 0.0268 - val_loss: 9.3057\n",
      "Epoch 5/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0306 - loss: 7.3584 - val_accuracy: 0.0268 - val_loss: 9.9377\n",
      "Epoch 6/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0248 - loss: 7.1148 - val_accuracy: 0.0242 - val_loss: 10.6579\n",
      "Epoch 7/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0283 - loss: 6.8752 - val_accuracy: 0.0228 - val_loss: 11.4792\n",
      "Epoch 8/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0326 - loss: 6.5420 - val_accuracy: 0.0174 - val_loss: 11.5787\n",
      "Epoch 9/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0329 - loss: 6.2797 - val_accuracy: 0.0107 - val_loss: 11.7907\n",
      "Epoch 10/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0236 - loss: 6.0957 - val_accuracy: 0.0094 - val_loss: 12.2534\n",
      "Epoch 11/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0306 - loss: 5.8334 - val_accuracy: 0.0121 - val_loss: 12.7127\n",
      "Epoch 12/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0317 - loss: 5.6528 - val_accuracy: 0.0081 - val_loss: 12.6968\n",
      "Epoch 13/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0367 - loss: 5.4595 - val_accuracy: 0.0054 - val_loss: 13.2309\n",
      "Epoch 14/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0462 - loss: 5.2748 - val_accuracy: 0.0054 - val_loss: 13.1182\n",
      "Epoch 15/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0411 - loss: 5.1510 - val_accuracy: 0.0054 - val_loss: 13.7335\n",
      "Epoch 16/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.0475 - loss: 4.9534 - val_accuracy: 0.0067 - val_loss: 14.0273\n",
      "Epoch 17/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0556 - loss: 4.7926 - val_accuracy: 0.0040 - val_loss: 14.4704\n",
      "Epoch 18/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0572 - loss: 4.6948 - val_accuracy: 0.0067 - val_loss: 14.3309\n",
      "Epoch 19/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0725 - loss: 4.5293 - val_accuracy: 0.0054 - val_loss: 14.6732\n",
      "Epoch 20/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0740 - loss: 4.3837 - val_accuracy: 0.0067 - val_loss: 14.9266\n",
      "Epoch 21/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0813 - loss: 4.2406 - val_accuracy: 0.0054 - val_loss: 15.1801\n",
      "Epoch 22/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0938 - loss: 4.1393 - val_accuracy: 0.0040 - val_loss: 15.1578\n",
      "Epoch 23/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0968 - loss: 4.0538 - val_accuracy: 0.0067 - val_loss: 15.6603\n",
      "Epoch 24/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0933 - loss: 3.9506 - val_accuracy: 0.0054 - val_loss: 15.8204\n",
      "Epoch 25/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.1118 - loss: 3.8238 - val_accuracy: 0.0067 - val_loss: 16.2176\n",
      "Epoch 26/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.1266 - loss: 3.6887 - val_accuracy: 0.0067 - val_loss: 16.2569\n",
      "Epoch 27/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.1562 - loss: 3.5702 - val_accuracy: 0.0040 - val_loss: 16.6739\n",
      "Epoch 28/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.1546 - loss: 3.4954 - val_accuracy: 0.0040 - val_loss: 16.9589\n",
      "Epoch 29/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.1834 - loss: 3.4390 - val_accuracy: 0.0067 - val_loss: 17.1699\n",
      "Epoch 30/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.1946 - loss: 3.3117 - val_accuracy: 0.0054 - val_loss: 17.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete. Model saved!\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step\n",
      "ğŸ¤– Chatbot: get ticket\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "ğŸ¤– Chatbot: world needs polite people like us\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "ğŸ¤– Chatbot: said need think positive\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "ğŸ¤– Chatbot: get ticket\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "ğŸ¤– Chatbot: get ticket\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "ğŸ¤– Chatbot: world needs polite people like us\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "ğŸ¤– Chatbot: get ticket\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "ğŸ¤– Chatbot: get ticket\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "ğŸ¤– Chatbot: get ticket\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = text.lower().strip()  # Convert to lowercase\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# Encode responses using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"response_encoded\"] = label_encoder.fit_transform(df[\"clean_response\"])\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"clean_question\"])\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_question\"])\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(df[\"response_encoded\"])\n",
    "\n",
    "# Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Load GloVe Embeddings\n",
    "embedding_dim = 300  # Use 300-dimensional GloVe\n",
    "glove_path = \"/Users/neha/Documents/chatbot/glove.6B.300d.txt\"\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = 5000  # Must match tokenizer's num_words\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "print(\"âœ… GloVe embeddings loaded successfully!\")\n",
    "\n",
    "# Define LSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=300, input_length=max_len,\n",
    "              weights=[embedding_matrix], trainable=False),  # Use GloVe embeddings\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")  # Output layer\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_val, y_val))\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_chatbot.h5\")\n",
    "\n",
    "# Save tokenizer and label encoder\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"âœ… Training complete. Model saved!\")\n",
    "\n",
    "# Chatbot Function\n",
    "def chatbot():\n",
    "    print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Preprocess input\n",
    "        user_input_cleaned = clean_text(user_input)\n",
    "        user_input_sequence = tokenizer.texts_to_sequences([user_input_cleaned])\n",
    "        user_input_padded = pad_sequences(user_input_sequence, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "        # Predict intent\n",
    "        predicted_intent_index = np.argmax(model.predict(user_input_padded))\n",
    "        predicted_intent = label_encoder.inverse_transform([predicted_intent_index])[0]\n",
    "\n",
    "        print(f\"ğŸ¤– Chatbot: {predicted_intent}\")\n",
    "\n",
    "# Run chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common responses:\n",
      " clean_response\n",
      "                111\n",
      "yes              26\n",
      "mean             23\n",
      "like             18\n",
      "course           16\n",
      "sure             14\n",
      "say              11\n",
      "happened         11\n",
      "whats matter     10\n",
      "really           10\n",
      "Name: count, dtype: int64\n",
      "âœ… GloVe embeddings loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - accuracy: 0.0000e+00 - loss: 8.0962 - val_accuracy: 0.0000e+00 - val_loss: 8.0484\n",
      "Epoch 2/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 73ms/step - accuracy: 0.0000e+00 - loss: 8.0877 - val_accuracy: 0.0014 - val_loss: 8.2380\n",
      "Epoch 3/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 67ms/step - accuracy: 0.0012 - loss: 8.0065 - val_accuracy: 0.0000e+00 - val_loss: 8.8114\n",
      "Epoch 4/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.0013 - loss: 7.9233 - val_accuracy: 0.0000e+00 - val_loss: 9.0743\n",
      "Epoch 5/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - accuracy: 0.0025 - loss: 7.7900 - val_accuracy: 0.0014 - val_loss: 9.3395\n",
      "Epoch 6/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 67ms/step - accuracy: 0.0013 - loss: 7.6345 - val_accuracy: 0.0000e+00 - val_loss: 9.5702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete. Model saved!\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step\n",
      "ğŸ¤– Chatbot (Best guess): die\n",
      "ğŸ¤– Chatbot (Other possible responses): ['course' 'sure']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "ğŸ¤– Chatbot (Best guess): die\n",
      "ğŸ¤– Chatbot (Other possible responses): ['well corporations think money' 'still miss']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "ğŸ¤– Chatbot (Best guess): course\n",
      "ğŸ¤– Chatbot (Other possible responses): ['long take' 'sure']\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Clean and Balance Dataset\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Check for Overrepresented Responses\n",
    "print(\"Most common responses:\\n\", df[\"clean_response\"].value_counts().head(10))\n",
    "\n",
    "# If \"get ticket\" appears too much, drop some instances (Example: keep only 5 occurrences)\n",
    "df = df.groupby(\"clean_response\").head(5)\n",
    "\n",
    "# âœ… Step 4: Encode Responses\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"response_encoded\"] = label_encoder.fit_transform(df[\"clean_response\"])\n",
    "\n",
    "# âœ… Step 5: Tokenize Text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")  # Increased vocab size\n",
    "tokenizer.fit_on_texts(df[\"clean_question\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_question\"])\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "labels = np.array(df[\"response_encoded\"])\n",
    "\n",
    "# âœ… Step 6: Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Step 7: Load GloVe Embeddings\n",
    "embedding_dim = 300\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "embedding_matrix = np.zeros((10000, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < 10000:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "print(\"âœ… GloVe embeddings loaded successfully!\")\n",
    "\n",
    "# âœ… Step 8: Define LSTM Model (Bidirectional LSTM for better understanding)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=300, input_length=max_len,\n",
    "              weights=[embedding_matrix], trainable=False),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),  # Bidirectional LSTM\n",
    "    LSTM(64),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# âœ… Step 9: Compile Model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# âœ… Step 10: Train Model with Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# âœ… Step 11: Save Model & Tokenizer\n",
    "model.save(\"lstm_chatbot.h5\")\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"âœ… Training complete. Model saved!\")\n",
    "\n",
    "# âœ… Step 12: Define Chatbot\n",
    "def chatbot():\n",
    "    print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Preprocess input\n",
    "        user_input_cleaned = clean_text(user_input)\n",
    "        user_input_sequence = tokenizer.texts_to_sequences([user_input_cleaned])\n",
    "        user_input_padded = pad_sequences(user_input_sequence, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "        # Predict intent\n",
    "        predictions = model.predict(user_input_padded)\n",
    "        top_k = 3  # Get top 3 responses\n",
    "        top_k_indices = predictions.argsort()[0][-top_k:][::-1]  # Get indices of top 3 responses\n",
    "        predicted_intents = label_encoder.inverse_transform(top_k_indices)\n",
    "\n",
    "        print(f\"ğŸ¤– Chatbot (Best guess): {predicted_intents[0]}\")\n",
    "        print(f\"ğŸ¤– Chatbot (Other possible responses): {predicted_intents[1:]}\")\n",
    "\n",
    "\n",
    "# âœ… Step 13: Run Chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.28.0)\n",
      "Requirement already satisfied: Pillow in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common responses:\n",
      " clean_response\n",
      "                111\n",
      "yes              26\n",
      "mean             23\n",
      "like             18\n",
      "course           16\n",
      "sure             14\n",
      "say              11\n",
      "happened         11\n",
      "whats matter     10\n",
      "really           10\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10ee40f64744b38bc9f5d886e527104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete. Model saved!\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: bites\n",
      "ğŸ¤– Chatbot: bites\n",
      "ğŸ¤– Chatbot: didnt know\n",
      "ğŸ¤– Chatbot: california\n",
      "ğŸ¤– Chatbot: bites\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Clean and Balance Dataset\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Check if Responses Are Overrepresented\n",
    "print(\"Most common responses:\\n\", df[\"clean_response\"].value_counts().head(10))\n",
    "\n",
    "# If \"get ticket\" appears too much, remove excessive occurrences\n",
    "df = df.groupby(\"clean_response\").head(5)\n",
    "\n",
    "# âœ… Step 4: Encode Responses\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"response_encoded\"] = label_encoder.fit_transform(df[\"clean_response\"])\n",
    "\n",
    "# âœ… Step 5: Load Pretrained Sentence Transformer (SBERT)\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert text to sentence embeddings\n",
    "X_embeddings = sbert_model.encode(df[\"clean_question\"].tolist(), show_progress_bar=True)\n",
    "y_labels = df[\"response_encoded\"].values\n",
    "\n",
    "# âœ… Step 6: Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_embeddings, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Step 7: Train a KNN Classifier (Instead of LSTM)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# âœ… Step 8: Save Model and Encoders\n",
    "pickle.dump(knn_model, open(\"sbert_knn_chatbot.pkl\", \"wb\"))\n",
    "pickle.dump(label_encoder, open(\"label_encoder.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Training complete. Model saved!\")\n",
    "\n",
    "# âœ… Step 9: Define Chatbot\n",
    "def chatbot():\n",
    "    print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Convert input to embedding\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "\n",
    "        # Predict best matching response\n",
    "        predicted_label = knn_model.predict(user_embedding)[0]\n",
    "        predicted_response = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "      print(f\"ğŸ¤– Chatbot: {predicted_response}\")\n",
    "\n",
    "# âœ… Step 10: Run Chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db24703aed6c404c97d1fdf0b5b30ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete. Model saved!\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: im fine\n",
      "ğŸ¤– Chatbot: basically ive working\n",
      "ğŸ¤– Chatbot: yes expensive\n",
      "ğŸ¤– Chatbot: yes expensive\n",
      "ğŸ¤– Chatbot: eat much chocolate\n",
      "ğŸ¤– Chatbot: good hope doesnt cool weekend\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Train SBERT Model (Better Version)\n",
    "sbert_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert questions to embeddings\n",
    "X_embeddings = sbert_model.encode(df[\"clean_question\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# âœ… Step 4: Save Data for Fast Retrieval\n",
    "pickle.dump(X_embeddings, open(\"X_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Training complete. Model saved!\")\n",
    "\n",
    "# âœ… Step 5: Define Chatbot with Cosine Similarity\n",
    "def chatbot():\n",
    "    print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Convert input to embedding\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "\n",
    "        # Load saved embeddings & responses\n",
    "        X_embeddings = pickle.load(open(\"X_embeddings.pkl\", \"rb\"))\n",
    "        responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "\n",
    "        # Find best match using cosine similarity\n",
    "        similarities = cosine_similarity(user_embedding, X_embeddings)\n",
    "        best_match_index = similarities.argmax()\n",
    "        best_response = responses[best_match_index]\n",
    "\n",
    "        print(f\"ğŸ¤– Chatbot: {best_response}\")\n",
    "\n",
    "# âœ… Step 6: Run Chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e578618799784b6a99bfb83d6d227c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19e07dd854a4d57bb38135bdfc86428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558a256dbf6c498fb44d757f3da5a705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114be22bc9874a6d982527ba29964683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576db8f540d14b84ba0e03c19be778ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897e63b9b7d483587b9ec35cf266d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421c1c8a48224e1faa79fac6acdcae37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e0a2818fe943118636e184380aab93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6db7888fe6c4202b520c7a9aca8bfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3006e1c082e4661912f4b066d2c69dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ba412a47ac4f669938e18fa2d98951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a880bb5bab7f45de886e2b8aa2d1c7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete. Model saved!\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: im fine\n",
      "ğŸ¤– Chatbot: I'm not sure how to answer that. Can you rephrase?\n",
      "ğŸ¤– Chatbot: favorite movie superbad\n",
      "ğŸ¤– Chatbot: favorite movie superbad\n",
      "ğŸ¤– Chatbot: hot sunny every day\n",
      "ğŸ¤– Chatbot: hot sunny every day\n",
      "ğŸ¤– Chatbot: calling\n",
      "ğŸ¤– Chatbot: yes expensive\n",
      "ğŸ¤– Chatbot: yes expensive\n",
      "ğŸ¤– Chatbot: I'm not sure how to answer that. Can you rephrase?\n",
      "ğŸ¤– Chatbot: going\n",
      "ğŸ¤– Chatbot: going\n",
      "ğŸ¤– Chatbot: cops find flew us\n",
      "ğŸ¤– Chatbot: cops find flew us\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Use a Better SBERT Model for Q&A Tasks\n",
    "sbert_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")  \n",
    "\n",
    "# Convert questions to embeddings\n",
    "X_embeddings = sbert_model.encode(df[\"clean_question\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# âœ… Step 4: Save Data for Fast Retrieval\n",
    "pickle.dump(X_embeddings, open(\"X_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Training complete. Model saved!\")\n",
    "\n",
    "# âœ… Step 5: Chatbot Function with Similarity Threshold\n",
    "def chatbot():\n",
    "    print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Convert input to embedding\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "\n",
    "        # Load saved embeddings & responses\n",
    "        X_embeddings = pickle.load(open(\"X_embeddings.pkl\", \"rb\"))\n",
    "        responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "\n",
    "        # Find best match using cosine similarity\n",
    "        similarities = cosine_similarity(user_embedding, X_embeddings)\n",
    "        best_match_index = similarities.argmax()\n",
    "        best_response = responses[best_match_index]\n",
    "        best_score = similarities[0, best_match_index]\n",
    "\n",
    "        # âœ… Add a Threshold to Avoid Incorrect Responses\n",
    "        if best_score < 0.5:  # If similarity is too low, don't return a random response\n",
    "            print(\"ğŸ¤– Chatbot: I'm not sure how to answer that. Can you rephrase?\")\n",
    "        else:\n",
    "            print(f\"ğŸ¤– Chatbot: {best_response}\")\n",
    "\n",
    "# âœ… Step 6: Run Chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0296ce3f9c584e3fa81e2c1b21a40d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete. Model saved!\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: im fine\n",
      "ğŸ¤– Chatbot: im fine\n",
      "ğŸ¤– Chatbot: im well\n",
      "ğŸ¤– Chatbot: im fine\n",
      "ğŸ¤– Chatbot: im well\n",
      "ğŸ¤– Chatbot: bad lot people\n",
      "ğŸ¤– Chatbot: im fine\n",
      "ğŸ¤– Chatbot: cousin went labor baby last week\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import deque\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = text.lower().strip()  # Convert to lowercase\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Use SBERT Model for Semantic Understanding\n",
    "sbert_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# Convert questions to embeddings\n",
    "X_embeddings = sbert_model.encode(df[\"clean_question\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# âœ… Step 4: Save Data for Fast Retrieval\n",
    "pickle.dump(X_embeddings, open(\"X_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Training complete. Model saved!\")\n",
    "\n",
    "# âœ… Step 5: Chatbot with Conversational Memory\n",
    "class ChatbotWithMemory:\n",
    "    def __init__(self):\n",
    "        self.sbert_model = sbert_model\n",
    "        self.X_embeddings = pickle.load(open(\"X_embeddings.pkl\", \"rb\"))\n",
    "        self.responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "        self.questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "        self.memory = deque(maxlen=5)  # Store last 5 conversations\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        user_embedding = self.sbert_model.encode([user_input])\n",
    "        similarities = cosine_similarity(user_embedding, self.X_embeddings)\n",
    "        best_match_index = similarities.argmax()\n",
    "        best_response = self.responses[best_match_index]\n",
    "        best_score = similarities[0, best_match_index]\n",
    "\n",
    "        # âœ… Context Handling - Search Memory for Related Responses\n",
    "        for past_input, past_response in self.memory:\n",
    "            if past_input in user_input or user_input in past_response:\n",
    "                return past_response  # Return relevant past response\n",
    "\n",
    "        # âœ… Avoid Irrelevant Responses (Set Threshold)\n",
    "        if best_score < 0.4:\n",
    "            return \"I'm not sure how to answer that. Can you rephrase?\"\n",
    "\n",
    "        # âœ… Save Conversation in Memory\n",
    "        self.memory.append((user_input, best_response))\n",
    "\n",
    "        return best_response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip().lower()\n",
    "            if user_input == \"exit\":\n",
    "                print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            best_response = self.get_best_response(user_input)\n",
    "            print(f\"ğŸ¤– Chatbot: {best_response}\")\n",
    "\n",
    "# âœ… Step 6: Run Chatbot\n",
    "chatbot = ChatbotWithMemory()\n",
    "chatbot.chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/neha/Library/Python/3.9/lib/python/site-packages (4.48.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading faiss_cpu-1.10.0-cp39-cp39-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp39-cp39-macosx_10_9_universal2.whl size=169877 sha256=9f49c36f6b8e77c9391bebf6835b1fa1690cf10b36c78a7189abf60068409d4d\n",
      "  Stored in directory: /Users/neha/Library/Caches/pip/wheels/f2/7a/49/9bef8878949914ecb90c08fc5bf30a05e17f475fe7e08b63a8\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0 pickle5-0.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers faiss-cpu nltk pandas numpy pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>hi, doing?</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "      <td>problem. been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>problem. been?</td>\n",
       "      <td>i've great. you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "      <td>i've great. you?</td>\n",
       "      <td>i've good. i'm school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                   response                   clean_question  \\\n",
       "0             i'm fine. how about yourself?                       hi, doing?   \n",
       "1       i'm pretty good. thanks for asking.              i'm fine. yourself?   \n",
       "2         no problem. so how have you been?  i'm pretty good. thanks asking.   \n",
       "3          i've been great. what about you?                   problem. been?   \n",
       "4  i've been good. i'm in school right now.                 i've great. you?   \n",
       "\n",
       "                     clean_response  \n",
       "0               i'm fine. yourself?  \n",
       "1   i'm pretty good. thanks asking.  \n",
       "2                    problem. been?  \n",
       "3                  i've great. you?  \n",
       "4  i've good. i'm school right now.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# âœ… Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# âœ… Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Show the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# âœ… Load Transformer Model for Sentence Embeddings (Offline)\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# âœ… Function to convert sentences to embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "# âœ… Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([get_embedding(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# âœ… Save Data for Offline Use\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# âœ… Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: i'm fine. yourself?\n",
      "ğŸ¤– Chatbot: i'm pretty good. thanks asking.\n",
      "ğŸ¤– Chatbot: tall.\n",
      "ğŸ¤– Chatbot: nothing, you?\n",
      "ğŸ¤– Chatbot: things, besides, polite nosey.\n",
      "ğŸ¤– Chatbot: things, besides, polite nosey.\n",
      "ğŸ¤– Chatbot: want get guns street.\n",
      "ğŸ¤– Chatbot: want get guns street.\n",
      "ğŸ¤– Chatbot: next game, definitely there.\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# âœ… Load Saved Data\n",
    "index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))\n",
    "responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "\n",
    "# âœ… Chatbot Memory (Stores Last 5 Conversations)\n",
    "chat_memory = deque(maxlen=5)\n",
    "\n",
    "# âœ… Function to Find Best Response\n",
    "def get_best_response(user_input):\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    _, best_match_index = index.search(user_embedding, 1)\n",
    "    best_response = responses[best_match_index[0][0]]\n",
    "\n",
    "    # âœ… Check Memory for Context\n",
    "    for past_input, past_response in chat_memory:\n",
    "        if past_input in user_input or user_input in past_response:\n",
    "            return past_response  # Return relevant past response\n",
    "\n",
    "    # âœ… Store Conversation in Memory\n",
    "    chat_memory.append((user_input, best_response))\n",
    "\n",
    "    return best_response\n",
    "\n",
    "# âœ… Interactive Chatbot in Jupyter Notebook\n",
    "def chat():\n",
    "    print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = get_best_response(user_input)\n",
    "        print(f\"ğŸ¤– Chatbot: {response}\")\n",
    "\n",
    "# âœ… Run Chatbot\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/neha/Library/Python/3.9/lib/python/site-packages (4.48.2)\n",
      "Requirement already satisfied: faiss-cpu in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.10.0)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: pickle5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (0.0.11)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers faiss-cpu nltk pandas numpy pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>hi, doing?</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "      <td>problem. been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>problem. been?</td>\n",
       "      <td>i've great. you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "      <td>i've great. you?</td>\n",
       "      <td>i've good. i'm school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                   response                   clean_question  \\\n",
       "0             i'm fine. how about yourself?                       hi, doing?   \n",
       "1       i'm pretty good. thanks for asking.              i'm fine. yourself?   \n",
       "2         no problem. so how have you been?  i'm pretty good. thanks asking.   \n",
       "3          i've been great. what about you?                   problem. been?   \n",
       "4  i've been good. i'm in school right now.                 i've great. you?   \n",
       "\n",
       "                     clean_response  \n",
       "0               i'm fine. yourself?  \n",
       "1   i'm pretty good. thanks asking.  \n",
       "2                    problem. been?  \n",
       "3                  i've great. you?  \n",
       "4  i've good. i'm school right now.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# âœ… Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# âœ… Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Show the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# âœ… Load Transformer Model for Sentence Embeddings (Offline)\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# âœ… Function to convert sentences to embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "# âœ… Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([get_embedding(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# âœ… Save Data for Offline Use\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# âœ… Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>hi, doing?</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "      <td>problem. been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>problem. been?</td>\n",
       "      <td>i've great. you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "      <td>i've great. you?</td>\n",
       "      <td>i've good. i'm school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                   response                   clean_question  \\\n",
       "0             i'm fine. how about yourself?                       hi, doing?   \n",
       "1       i'm pretty good. thanks for asking.              i'm fine. yourself?   \n",
       "2         no problem. so how have you been?  i'm pretty good. thanks asking.   \n",
       "3          i've been great. what about you?                   problem. been?   \n",
       "4  i've been good. i'm in school right now.                 i've great. you?   \n",
       "\n",
       "                     clean_response  \n",
       "0               i'm fine. yourself?  \n",
       "1   i'm pretty good. thanks asking.  \n",
       "2                    problem. been?  \n",
       "3                  i've great. you?  \n",
       "4  i've good. i'm school right now.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# âœ… Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# âœ… Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Show the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# âœ… Load a proper embedding model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# âœ… Function to extract embeddings correctly\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0, :].numpy()  # Correct embedding extraction\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# âœ… Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([get_embedding(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# âœ… Save Data for Offline Use\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# âœ… Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/neha/Library/Python/3.9/lib/python/site-packages (4.48.2)\n",
      "Requirement already satisfied: faiss-cpu in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.10.0)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: pickle5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (0.0.11)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers faiss-cpu nltk pandas numpy pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete! Model saved for offline use.\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import deque\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Use Sentence Transformer for Embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# âœ… Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([sbert_model.encode(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# âœ… Save Embeddings Using FAISS for Fast Retrieval\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# âœ… Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Training complete! Model saved for offline use.\")\n",
    "\n",
    "# âœ… Step 4: Chatbot with Memory\n",
    "class ChatbotWithMemory:\n",
    "    def __init__(self):\n",
    "        self.sbert_model = sbert_model\n",
    "        self.index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))\n",
    "        self.responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "        self.questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "        self.memory = deque(maxlen=10)  # Stores last 10 conversations\n",
    "        self.memory_dict = {}  # Stores user preferences, name, etc.\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        \"\"\"Finds the most relevant response from the dataset using FAISS.\"\"\"\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "        _, best_match_index = self.index.search(user_embedding, 1)\n",
    "        best_response = self.responses[best_match_index[0][0]]\n",
    "\n",
    "        # âœ… Context Handling - Check Memory for Related Responses\n",
    "        for past_input, past_response in self.memory:\n",
    "            if past_input in user_input or user_input in past_response:\n",
    "                return past_response  # Return a relevant past response\n",
    "\n",
    "        # âœ… Store the conversation in memory\n",
    "        self.memory.append((user_input, best_response))\n",
    "\n",
    "        return best_response\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chatbot function.\"\"\"\n",
    "        print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip().lower()\n",
    "            if user_input == \"exit\":\n",
    "                print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # âœ… Store User Details Dynamically\n",
    "            if \"my name is\" in user_input:\n",
    "                name = user_input.split(\"my name is\")[-1].strip()\n",
    "                self.memory_dict[\"name\"] = name\n",
    "                print(f\"ğŸ¤– Chatbot: Nice to meet you, {name}!\")\n",
    "                continue\n",
    "\n",
    "            elif \"what is my name\" in user_input and \"name\" in self.memory_dict:\n",
    "                print(f\"ğŸ¤– Chatbot: Your name is {self.memory_dict['name']}.\")\n",
    "                continue\n",
    "\n",
    "            elif \"i like\" in user_input:\n",
    "                preference = user_input.split(\"i like\")[-1].strip()\n",
    "                self.memory_dict[\"preference\"] = preference\n",
    "                print(f\"ğŸ¤– Chatbot: Great! Ill remember that you like {preference}.\")\n",
    "                continue\n",
    "\n",
    "            elif \"what do i like\" in user_input and \"preference\" in self.memory_dict:\n",
    "                print(f\"ğŸ¤– Chatbot: You like {self.memory_dict['preference']}!\")\n",
    "                continue\n",
    "\n",
    "            # âœ… Retrieve the Best Response\n",
    "            best_response = self.get_best_response(user_input)\n",
    "\n",
    "            print(f\"ğŸ¤– Chatbot: {best_response}\")\n",
    "\n",
    "# âœ… Run Chatbot\n",
    "chatbot = ChatbotWithMemory()\n",
    "chatbot.chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete! Model saved for offline use.\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: I'm not sure how to answer that. Can you ask differently?\n",
      "ğŸ¤– Chatbot: I'm not sure how to answer that. Can you ask differently?\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import deque\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Use Sentence Transformer for Embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# âœ… Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([sbert_model.encode(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# âœ… Save Embeddings Using FAISS for Fast Retrieval\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# âœ… Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Training complete! Model saved for offline use.\")\n",
    "\n",
    "# âœ… Step 4: Chatbot with Correct Response Retrieval\n",
    "class ChatbotWithMemory:\n",
    "    def __init__(self):\n",
    "        self.sbert_model = sbert_model\n",
    "        self.index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))\n",
    "        self.responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "        self.questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "        self.memory = deque(maxlen=10)  # Stores last 10 conversations\n",
    "        self.memory_dict = {}  # Stores user preferences, name, etc.\n",
    "        self.last_response = None  # Prevent response repetition\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        \"\"\"Finds the most relevant response using FAISS, ensuring it is meaningful.\"\"\"\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "        distances, best_match_index = self.index.search(user_embedding, 3)  # Get top 3 best matches\n",
    "        possible_responses = [self.responses[i] for i in best_match_index[0]]\n",
    "        scores = distances[0]\n",
    "\n",
    "        # âœ… Ensure meaningful responses (avoid bad matches)\n",
    "        for response, score in zip(possible_responses, scores):\n",
    "            if response != self.last_response and response.strip() != \"\" and score < 0.4:\n",
    "                self.last_response = response\n",
    "                return response\n",
    "\n",
    "        # âœ… If all responses are similar, return a fallback response\n",
    "        return \"I'm not sure how to answer that. Can you ask differently?\"\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chatbot function.\"\"\"\n",
    "        print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip().lower()\n",
    "            if user_input == \"exit\":\n",
    "                print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # âœ… Ignore empty or invalid input\n",
    "            if user_input in [\"\", \".\", \"?\", \"!\"]:\n",
    "                print(\"ğŸ¤– Chatbot: Could you provide more details?\")\n",
    "                continue\n",
    "\n",
    "            # âœ… Retrieve the Best Response from `dialogs.txt`\n",
    "            best_response = self.get_best_response(user_input)\n",
    "\n",
    "            print(f\"ğŸ¤– Chatbot: {best_response}\")\n",
    "\n",
    "# âœ… Run Chatbot\n",
    "chatbot = ChatbotWithMemory()\n",
    "chatbot.chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete! Model saved for offline use.\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: i'm fine. yourself?\n",
      "ğŸ¤– Chatbot: Great! Iâ€™ll remember that you like .\n",
      "ğŸ¤– Chatbot: Nice to meet you, jitesh!\n",
      "ğŸ¤– Chatbot: i.\n",
      "ğŸ¤– Chatbot: practice every day.\n",
      "ğŸ¤– Chatbot: practice every day.\n",
      "ğŸ¤– Chatbot: kid.\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import deque\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Use Sentence Transformer for Embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# âœ… Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([sbert_model.encode(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# âœ… Save Embeddings Using FAISS for Fast Retrieval\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# âœ… Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Training complete! Model saved for offline use.\")\n",
    "\n",
    "# âœ… Step 4: Chatbot with Memory\n",
    "class ChatbotWithMemory:\n",
    "    def __init__(self):\n",
    "        self.sbert_model = sbert_model\n",
    "        self.index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))\n",
    "        self.responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "        self.questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "        self.memory = deque(maxlen=10)  # Stores last 10 conversations\n",
    "        self.memory_dict = {}  # Stores user preferences, name, etc.\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        \"\"\"Finds the most relevant response from the dataset using FAISS.\"\"\"\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "        _, best_match_index = self.index.search(user_embedding, 1)\n",
    "        best_response = self.responses[best_match_index[0][0]]\n",
    "\n",
    "        # âœ… Context Handling - Check Memory for Related Responses\n",
    "        for past_input, past_response in self.memory:\n",
    "            if past_input in user_input or user_input in past_response:\n",
    "                return past_response  # Return a relevant past response\n",
    "\n",
    "        # âœ… Store the conversation in memory\n",
    "        self.memory.append((user_input, best_response))\n",
    "\n",
    "        return best_response\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chatbot function.\"\"\"\n",
    "        print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip().lower()\n",
    "            if user_input == \"exit\":\n",
    "                print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # âœ… Store User Details Dynamically\n",
    "            if \"my name is\" in user_input:\n",
    "                name = user_input.split(\"my name is\")[-1].strip()\n",
    "                self.memory_dict[\"name\"] = name\n",
    "                print(f\"ğŸ¤– Chatbot: Nice to meet you, {name}!\")\n",
    "                continue\n",
    "\n",
    "            elif \"what is my name\" in user_input and \"name\" in self.memory_dict:\n",
    "                print(f\"ğŸ¤– Chatbot: Your name is {self.memory_dict['name']}.\")\n",
    "                continue\n",
    "\n",
    "            elif \"i like\" in user_input:\n",
    "                preference = user_input.split(\"i like\")[-1].strip()\n",
    "                self.memory_dict[\"preference\"] = preference\n",
    "                print(f\"ğŸ¤– Chatbot: Great! Iâ€™ll remember that you like {preference}.\")\n",
    "                continue\n",
    "\n",
    "            elif \"what do i like\" in user_input and \"preference\" in self.memory_dict:\n",
    "                print(f\"ğŸ¤– Chatbot: You like {self.memory_dict['preference']}!\")\n",
    "                continue\n",
    "\n",
    "            # âœ… Retrieve the Best Response\n",
    "            best_response = self.get_best_response(user_input)\n",
    "\n",
    "            print(f\"ğŸ¤– Chatbot: {best_response}\")\n",
    "\n",
    "# âœ… Run Chatbot\n",
    "chatbot = ChatbotWithMemory()\n",
    "chatbot.chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 3725 lines from the file.\n",
      "hi, how are you doing?\ti'm fine. how about yourself?\n",
      "i'm fine. how about yourself?\ti'm pretty good. thanks for asking.\n",
      "i'm pretty good. thanks for asking.\tno problem. so how have you been?\n",
      "no problem. so how have you been?\ti've been great. what about you?\n",
      "i've been great. what about you?\ti've been good. i'm in school right now.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"  # Adjust if needed\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Check if file has content\n",
    "if not lines:\n",
    "    print(\"âŒ Error: The file is empty!\")\n",
    "else:\n",
    "    print(f\"âœ… Loaded {len(lines)} lines from the file.\")\n",
    "\n",
    "# âœ… Display a few lines\n",
    "for i in range(5):  # Show first 5 lines\n",
    "    print(lines[i].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              question  \\\n",
      "0               hi, how are you doing?   \n",
      "1        i'm fine. how about yourself?   \n",
      "2  i'm pretty good. thanks for asking.   \n",
      "3    no problem. so how have you been?   \n",
      "4     i've been great. what about you?   \n",
      "\n",
      "                                   response  \n",
      "0             i'm fine. how about yourself?  \n",
      "1       i'm pretty good. thanks for asking.  \n",
      "2         no problem. so how have you been?  \n",
      "3          i've been great. what about you?  \n",
      "4  i've been good. i'm in school right now.  \n"
     ]
    }
   ],
   "source": [
    "# âœ… Check if the dataset is formatted correctly\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Show first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              question  \\\n",
      "0               hi, how are you doing?   \n",
      "1        i'm fine. how about yourself?   \n",
      "2  i'm pretty good. thanks for asking.   \n",
      "3    no problem. so how have you been?   \n",
      "4     i've been great. what about you?   \n",
      "\n",
      "                                   response                   clean_question  \\\n",
      "0             i'm fine. how about yourself?                       hi, doing?   \n",
      "1       i'm pretty good. thanks for asking.              i'm fine. yourself?   \n",
      "2         no problem. so how have you been?  i'm pretty good. thanks asking.   \n",
      "3          i've been great. what about you?                   problem. been?   \n",
      "4  i've been good. i'm in school right now.                 i've great. you?   \n",
      "\n",
      "                     clean_response  \n",
      "0               i'm fine. yourself?  \n",
      "1   i'm pretty good. thanks asking.  \n",
      "2                    problem. been?  \n",
      "3                  i've great. you?  \n",
      "4  i've good. i'm school right now.  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Lowercases and removes stopwords.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Remove empty values\n",
    "df = df.dropna(subset=[\"clean_question\", \"clean_response\"])\n",
    "df = df[df[\"clean_question\"].str.strip() != \"\"]\n",
    "\n",
    "# âœ… Show the final dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS Index trained successfully!\n",
      "ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: I'm not sure how to answer that. Can you rephrase your question?\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# âœ… Load a proper model for embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# âœ… Convert all questions to embeddings\n",
    "valid_questions = df[\"clean_question\"].tolist()\n",
    "question_embeddings = np.vstack([sbert_model.encode(q) for q in valid_questions if q.strip() != \"\"])\n",
    "\n",
    "# âœ… Ensure FAISS is trained properly\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# âœ… Save Data\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(valid_questions, open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… FAISS Index trained successfully!\")\n",
    "\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# âœ… Load Stored Data (Fixing File Mode)\n",
    "index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))  # Read mode\n",
    "responses = pickle.load(open(\"responses.pkl\", \"rb\"))  # Read mode\n",
    "questions = pickle.load(open(\"questions.pkl\", \"rb\"))  # âœ… Fixed to \"rb\"\n",
    "\n",
    "# âœ… Chatbot Memory (Stores Last 10 Conversations)\n",
    "chat_memory = deque(maxlen=10)\n",
    "\n",
    "# âœ… Function to Find Best Response\n",
    "def get_best_response(user_input):\n",
    "    user_embedding = sbert_model.encode([user_input])\n",
    "    distances, best_match_index = index.search(user_embedding, 3)  # Get top 3 best matches\n",
    "    possible_responses = [responses[i] for i in best_match_index[0]]\n",
    "    scores = distances[0]\n",
    "\n",
    "    # âœ… Ensure meaningful responses (avoid bad matches)\n",
    "    for response, score in zip(possible_responses, scores):\n",
    "        if response.strip() != \"\" and score < 0.5:  # Adjust threshold if needed\n",
    "            return response\n",
    "\n",
    "    # âœ… If no good match, ask user to clarify\n",
    "    return \"I'm not sure how to answer that. Can you rephrase your question?\"\n",
    "\n",
    "# âœ… Interactive Chatbot in Jupyter Notebook\n",
    "def chat():\n",
    "    print(\"ğŸ¤– Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # âœ… Ignore empty or invalid input\n",
    "        if user_input in [\"\", \".\", \"?\", \"!\"]:\n",
    "            print(\"ğŸ¤– Chatbot: Could you provide more details?\")\n",
    "            continue\n",
    "\n",
    "        # âœ… Retrieve the Best Response from `dialogs.txt`\n",
    "        best_response = get_best_response(user_input)\n",
    "\n",
    "        print(f\"ğŸ¤– Chatbot: {best_response}\")\n",
    "\n",
    "# âœ… Run Chatbot\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Chatbot Model...\n",
      "Epoch 1/20 - Loss: 8.3245\n",
      "Epoch 2/20 - Loss: 7.9115\n",
      "Epoch 3/20 - Loss: 7.7390\n",
      "Epoch 4/20 - Loss: 7.1597\n",
      "Epoch 5/20 - Loss: 6.3278\n",
      "Epoch 6/20 - Loss: 5.9846\n",
      "Epoch 7/20 - Loss: 5.1473\n",
      "Epoch 8/20 - Loss: 4.6981\n",
      "Epoch 9/20 - Loss: 3.8446\n",
      "Epoch 10/20 - Loss: 2.4549\n",
      "Epoch 11/20 - Loss: 1.6012\n",
      "Epoch 12/20 - Loss: 1.1400\n",
      "Epoch 13/20 - Loss: 1.0119\n",
      "Epoch 14/20 - Loss: 1.6997\n",
      "Epoch 15/20 - Loss: 0.8047\n",
      "Epoch 16/20 - Loss: 1.0525\n",
      "Epoch 17/20 - Loss: 0.8057\n",
      "Epoch 18/20 - Loss: 1.2940\n",
      "Epoch 19/20 - Loss: 1.7184\n",
      "Epoch 20/20 - Loss: 1.0626\n",
      "âœ… Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 2: Preprocess Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 3: Convert Text to Vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean_question\"]).toarray()\n",
    "y = df[\"clean_response\"].values\n",
    "\n",
    "# âœ… Step 4: Encode Responses\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# âœ… Save Tokenizer & Label Encoder\n",
    "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(label_encoder, open(\"label_encoder.pkl\", \"wb\"))\n",
    "\n",
    "# âœ… Step 5: Convert to PyTorch Dataset\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = ChatDataset(X, y_encoded)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# âœ… Step 6: Define RNN Model\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x.unsqueeze(1))\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "model = ChatbotModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# âœ… Step 7: Train Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"âœ… Training Chatbot Model...\")\n",
    "\n",
    "for epoch in range(20):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/20 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# âœ… Save Trained Model\n",
    "torch.save(model.state_dict(), \"chatbot_model.pth\")\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Chatbot Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 8.3839\n",
      "Epoch 2/10 - Loss: 8.1641\n",
      "Epoch 3/10 - Loss: 7.6838\n",
      "Epoch 4/10 - Loss: 7.1476\n",
      "Epoch 5/10 - Loss: 6.4886\n",
      "Epoch 6/10 - Loss: 5.1924\n",
      "Epoch 7/10 - Loss: 5.3963\n",
      "Epoch 8/10 - Loss: 3.8377\n",
      "Epoch 9/10 - Loss: 3.6412\n",
      "Epoch 10/10 - Loss: 3.6107\n",
      "âœ… Model trained and saved successfully!\n",
      "ğŸ¤– Chatbot: Hello! I remember past conversations. (Type 'exit' to stop)\n",
      "ğŸ¤– Chatbot: one. one that's black.\n",
      "ğŸ¤– Chatbot: would date someone even know?\n",
      "ğŸ¤– Chatbot: thank you. that's polite ask.\n",
      "ğŸ¤– Chatbot: would date someone even know?\n",
      "ğŸ¤– Chatbot: would date someone even know?\n",
      "ğŸ¤– Chatbot: put it?\n",
      "ğŸ¤– Chatbot: put it?\n",
      "ğŸ¤– Chatbot: law. can't keep jail forever.\n",
      "ğŸ¤– Chatbot: well, dozen large eggs 99 cents.\n",
      "ğŸ¤– Chatbot: gravity?\n",
      "ğŸ¤– Chatbot: know robber looks like?\n",
      "ğŸ¤– Chatbot: gravity?\n",
      "ğŸ¤– Chatbot: gravity?\n",
      "ğŸ¤– Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Install Required Libraries\n",
    "!pip install nltk torch scikit-learn pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"  # Ensure you upload this file in Colab\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# âœ… Step 2: Process Dataset\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# âœ… Step 3: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# âœ… Step 4: Convert Text to Vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean_question\"]).toarray()\n",
    "y = df[\"clean_response\"].values\n",
    "\n",
    "# âœ… Encode Responses\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# âœ… Save Tokenizer & Encoder\n",
    "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(label_encoder, open(\"label_encoder.pkl\", \"wb\"))\n",
    "\n",
    "# âœ… Step 5: Convert to PyTorch Dataset\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = ChatDataset(X, y_encoded)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# âœ… Step 6: Define LSTM Model\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x.unsqueeze(1))\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "model = ChatbotModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# âœ… Step 7: Train Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"âœ… Training Chatbot Model...\")\n",
    "\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/10 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# âœ… Save Trained Model\n",
    "torch.save(model.state_dict(), \"chatbot_model.pth\")\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Model trained and saved successfully!\")\n",
    "\n",
    "# âœ… Step 8: Load Model and Chat with AI\n",
    "vectorizer = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n",
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", \"rb\"))\n",
    "questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x.unsqueeze(1))\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "model = ChatbotModel(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(\"chatbot_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# âœ… Memory for Conversation Context\n",
    "memory = deque(maxlen=5)  # Stores the last 5 messages for context\n",
    "\n",
    "def get_best_response(user_input):\n",
    "    \"\"\"Predicts the best response using the trained model.\"\"\"\n",
    "    input_vector = vectorizer.transform([\" \".join(memory) + \" \" + user_input]).toarray()\n",
    "    input_tensor = torch.tensor(input_vector, dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    predicted_index = torch.argmax(output).item()\n",
    "    return responses[predicted_index]\n",
    "\n",
    "def chat():\n",
    "    print(\"ğŸ¤– Chatbot: Hello! I remember past conversations. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"ğŸ¤– Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # âœ… Store previous messages in memory\n",
    "        memory.append(user_input)\n",
    "\n",
    "        # âœ… Generate AI response\n",
    "        best_response = get_best_response(user_input)\n",
    "\n",
    "        print(f\"ğŸ¤– Chatbot: {best_response}\")\n",
    "\n",
    "# âœ… Run Chatbot\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
