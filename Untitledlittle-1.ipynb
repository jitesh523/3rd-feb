{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.16.2)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: rich in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/neha/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow pandas numpy scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (286472854.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    wget http://nlp.stanford.edu/data/glove.6B.zip\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4082407053.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python train_chatbot.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python train_chatbot.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m sequences \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(df[\u001b[39m\"\u001b[39m\u001b[39mclean_question\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Pad sequences\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m max_len \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(\u001b[39mlen\u001b[39;49m(seq) \u001b[39mfor\u001b[39;49;00m seq \u001b[39min\u001b[39;49;00m sequences)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m padded_sequences \u001b[39m=\u001b[39m pad_sequences(sequences, maxlen\u001b[39m=\u001b[39mmax_len, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Convert labels to numpy array\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = text.lower().strip()  # Convert to lowercase\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# Encode responses using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"response_encoded\"] = label_encoder.fit_transform(df[\"clean_response\"])\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"clean_question\"])\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_question\"])\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(df[\"response_encoded\"])\n",
    "\n",
    "# Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Load GloVe Embeddings\n",
    "embedding_dim = 300  # Use 300-dimensional GloVe\n",
    "glove_path = \"/Users/neha/Documents/chatbot/glove.6B.300d.txt\"\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = 5000  # Must match tokenizer's num_words\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "print(\"✅ GloVe embeddings loaded successfully!\")\n",
    "\n",
    "# Define LSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=300, input_length=max_len,\n",
    "              weights=[embedding_matrix], trainable=False),  # Use GloVe embeddings\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")  # Output layer\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_val, y_val))\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_chatbot.h5\")\n",
    "\n",
    "# Save tokenizer and label encoder\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"✅ Training complete. Model saved!\")\n",
    "\n",
    "# Chatbot Function\n",
    "def chatbot():\n",
    "    print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"🤖 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Preprocess input\n",
    "        user_input_cleaned = clean_text(user_input)\n",
    "        user_input_sequence = tokenizer.texts_to_sequences([user_input_cleaned])\n",
    "        user_input_padded = pad_sequences(user_input_sequence, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "        # Predict intent\n",
    "        predicted_intent_index = np.argmax(model.predict(user_input_padded))\n",
    "        predicted_intent = label_encoder.inverse_transform([predicted_intent_index])[0]\n",
    "\n",
    "        print(f\"🤖 Chatbot: {predicted_intent}\")\n",
    "\n",
    "# Run chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common responses:\n",
      " clean_response\n",
      "                111\n",
      "yes              26\n",
      "mean             23\n",
      "like             18\n",
      "course           16\n",
      "sure             14\n",
      "say              11\n",
      "happened         11\n",
      "whats matter     10\n",
      "really           10\n",
      "Name: count, dtype: int64\n",
      "✅ GloVe embeddings loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 51ms/step - accuracy: 0.0030 - loss: 8.0991 - val_accuracy: 0.0000e+00 - val_loss: 7.9647\n",
      "Epoch 2/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 46ms/step - accuracy: 8.7079e-04 - loss: 8.1222 - val_accuracy: 0.0000e+00 - val_loss: 8.4934\n",
      "Epoch 3/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 3.9158e-04 - loss: 8.0182 - val_accuracy: 0.0000e+00 - val_loss: 8.7582\n",
      "Epoch 4/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.0015 - loss: 7.9651 - val_accuracy: 0.0014 - val_loss: 9.3785\n",
      "Epoch 5/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - accuracy: 0.0015 - loss: 7.9080 - val_accuracy: 0.0000e+00 - val_loss: 9.5453\n",
      "Epoch 6/50\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.0021 - loss: 7.8775 - val_accuracy: 0.0000e+00 - val_loss: 9.9541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Model saved!\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "🤖 Chatbot (Best guess): writing letter\n",
      "🤖 Chatbot (Other possible responses): ['night long heard people snoring' 'play radio loud want']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🤖 Chatbot (Best guess): writing letter\n",
      "🤖 Chatbot (Other possible responses): ['night long heard people snoring' 'play radio loud want']\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Clean and Balance Dataset\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Check for Overrepresented Responses\n",
    "print(\"Most common responses:\\n\", df[\"clean_response\"].value_counts().head(10))\n",
    "\n",
    "# If \"get ticket\" appears too much, drop some instances (Example: keep only 5 occurrences)\n",
    "df = df.groupby(\"clean_response\").head(5)\n",
    "\n",
    "# ✅ Step 4: Encode Responses\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"response_encoded\"] = label_encoder.fit_transform(df[\"clean_response\"])\n",
    "\n",
    "# ✅ Step 5: Tokenize Text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")  # Increased vocab size\n",
    "tokenizer.fit_on_texts(df[\"clean_question\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_question\"])\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "labels = np.array(df[\"response_encoded\"])\n",
    "\n",
    "# ✅ Step 6: Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Step 7: Load GloVe Embeddings\n",
    "embedding_dim = 300\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "embedding_matrix = np.zeros((10000, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < 10000:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "print(\"✅ GloVe embeddings loaded successfully!\")\n",
    "\n",
    "# ✅ Step 8: Define LSTM Model (Bidirectional LSTM for better understanding)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=300, input_length=max_len,\n",
    "              weights=[embedding_matrix], trainable=False),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),  # Bidirectional LSTM\n",
    "    LSTM(64),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# ✅ Step 9: Compile Model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# ✅ Step 10: Train Model with Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# ✅ Step 11: Save Model & Tokenizer\n",
    "model.save(\"lstm_chatbot.h5\")\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"✅ Training complete. Model saved!\")\n",
    "\n",
    "# ✅ Step 12: Define Chatbot\n",
    "def chatbot():\n",
    "    print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"🤖 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Preprocess input\n",
    "        user_input_cleaned = clean_text(user_input)\n",
    "        user_input_sequence = tokenizer.texts_to_sequences([user_input_cleaned])\n",
    "        user_input_padded = pad_sequences(user_input_sequence, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "        # Predict intent\n",
    "        predictions = model.predict(user_input_padded)\n",
    "        top_k = 3  # Get top 3 responses\n",
    "        top_k_indices = predictions.argsort()[0][-top_k:][::-1]  # Get indices of top 3 responses\n",
    "        predicted_intents = label_encoder.inverse_transform(top_k_indices)\n",
    "\n",
    "        print(f\"🤖 Chatbot (Best guess): {predicted_intents[0]}\")\n",
    "        print(f\"🤖 Chatbot (Other possible responses): {predicted_intents[1:]}\")\n",
    "\n",
    "\n",
    "# ✅ Step 13: Run Chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.28.0)\n",
      "Requirement already satisfied: Pillow in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common responses:\n",
      " clean_response\n",
      "                111\n",
      "yes              26\n",
      "mean             23\n",
      "like             18\n",
      "course           16\n",
      "sure             14\n",
      "say              11\n",
      "happened         11\n",
      "whats matter     10\n",
      "really           10\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10ee40f64744b38bc9f5d886e527104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Model saved!\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: bites\n",
      "🤖 Chatbot: bites\n",
      "🤖 Chatbot: didnt know\n",
      "🤖 Chatbot: california\n",
      "🤖 Chatbot: bites\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Clean and Balance Dataset\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Check if Responses Are Overrepresented\n",
    "print(\"Most common responses:\\n\", df[\"clean_response\"].value_counts().head(10))\n",
    "\n",
    "# If \"get ticket\" appears too much, remove excessive occurrences\n",
    "df = df.groupby(\"clean_response\").head(5)\n",
    "\n",
    "# ✅ Step 4: Encode Responses\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"response_encoded\"] = label_encoder.fit_transform(df[\"clean_response\"])\n",
    "\n",
    "# ✅ Step 5: Load Pretrained Sentence Transformer (SBERT)\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert text to sentence embeddings\n",
    "X_embeddings = sbert_model.encode(df[\"clean_question\"].tolist(), show_progress_bar=True)\n",
    "y_labels = df[\"response_encoded\"].values\n",
    "\n",
    "# ✅ Step 6: Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_embeddings, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Step 7: Train a KNN Classifier (Instead of LSTM)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# ✅ Step 8: Save Model and Encoders\n",
    "pickle.dump(knn_model, open(\"sbert_knn_chatbot.pkl\", \"wb\"))\n",
    "pickle.dump(label_encoder, open(\"label_encoder.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Training complete. Model saved!\")\n",
    "\n",
    "# ✅ Step 9: Define Chatbot\n",
    "def chatbot():\n",
    "    print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"🤖 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Convert input to embedding\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "\n",
    "        # Predict best matching response\n",
    "        predicted_label = knn_model.predict(user_embedding)[0]\n",
    "        predicted_response = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "      print(f\"🤖 Chatbot: {predicted_response}\")\n",
    "\n",
    "# ✅ Step 10: Run Chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db24703aed6c404c97d1fdf0b5b30ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Model saved!\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: im fine\n",
      "🤖 Chatbot: basically ive working\n",
      "🤖 Chatbot: yes expensive\n",
      "🤖 Chatbot: yes expensive\n",
      "🤖 Chatbot: eat much chocolate\n",
      "🤖 Chatbot: good hope doesnt cool weekend\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Train SBERT Model (Better Version)\n",
    "sbert_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert questions to embeddings\n",
    "X_embeddings = sbert_model.encode(df[\"clean_question\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# ✅ Step 4: Save Data for Fast Retrieval\n",
    "pickle.dump(X_embeddings, open(\"X_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Training complete. Model saved!\")\n",
    "\n",
    "# ✅ Step 5: Define Chatbot with Cosine Similarity\n",
    "def chatbot():\n",
    "    print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"🤖 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Convert input to embedding\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "\n",
    "        # Load saved embeddings & responses\n",
    "        X_embeddings = pickle.load(open(\"X_embeddings.pkl\", \"rb\"))\n",
    "        responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "\n",
    "        # Find best match using cosine similarity\n",
    "        similarities = cosine_similarity(user_embedding, X_embeddings)\n",
    "        best_match_index = similarities.argmax()\n",
    "        best_response = responses[best_match_index]\n",
    "\n",
    "        print(f\"🤖 Chatbot: {best_response}\")\n",
    "\n",
    "# ✅ Step 6: Run Chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e578618799784b6a99bfb83d6d227c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19e07dd854a4d57bb38135bdfc86428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558a256dbf6c498fb44d757f3da5a705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114be22bc9874a6d982527ba29964683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576db8f540d14b84ba0e03c19be778ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897e63b9b7d483587b9ec35cf266d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421c1c8a48224e1faa79fac6acdcae37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e0a2818fe943118636e184380aab93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6db7888fe6c4202b520c7a9aca8bfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3006e1c082e4661912f4b066d2c69dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ba412a47ac4f669938e18fa2d98951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a880bb5bab7f45de886e2b8aa2d1c7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Model saved!\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: im fine\n",
      "🤖 Chatbot: I'm not sure how to answer that. Can you rephrase?\n",
      "🤖 Chatbot: favorite movie superbad\n",
      "🤖 Chatbot: favorite movie superbad\n",
      "🤖 Chatbot: hot sunny every day\n",
      "🤖 Chatbot: hot sunny every day\n",
      "🤖 Chatbot: calling\n",
      "🤖 Chatbot: yes expensive\n",
      "🤖 Chatbot: yes expensive\n",
      "🤖 Chatbot: I'm not sure how to answer that. Can you rephrase?\n",
      "🤖 Chatbot: going\n",
      "🤖 Chatbot: going\n",
      "🤖 Chatbot: cops find flew us\n",
      "🤖 Chatbot: cops find flew us\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Use a Better SBERT Model for Q&A Tasks\n",
    "sbert_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")  \n",
    "\n",
    "# Convert questions to embeddings\n",
    "X_embeddings = sbert_model.encode(df[\"clean_question\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# ✅ Step 4: Save Data for Fast Retrieval\n",
    "pickle.dump(X_embeddings, open(\"X_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Training complete. Model saved!\")\n",
    "\n",
    "# ✅ Step 5: Chatbot Function with Similarity Threshold\n",
    "def chatbot():\n",
    "    print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"🤖 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Convert input to embedding\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "\n",
    "        # Load saved embeddings & responses\n",
    "        X_embeddings = pickle.load(open(\"X_embeddings.pkl\", \"rb\"))\n",
    "        responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "\n",
    "        # Find best match using cosine similarity\n",
    "        similarities = cosine_similarity(user_embedding, X_embeddings)\n",
    "        best_match_index = similarities.argmax()\n",
    "        best_response = responses[best_match_index]\n",
    "        best_score = similarities[0, best_match_index]\n",
    "\n",
    "        # ✅ Add a Threshold to Avoid Incorrect Responses\n",
    "        if best_score < 0.5:  # If similarity is too low, don't return a random response\n",
    "            print(\"🤖 Chatbot: I'm not sure how to answer that. Can you rephrase?\")\n",
    "        else:\n",
    "            print(f\"🤖 Chatbot: {best_response}\")\n",
    "\n",
    "# ✅ Step 6: Run Chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0296ce3f9c584e3fa81e2c1b21a40d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Model saved!\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: im fine\n",
      "🤖 Chatbot: im fine\n",
      "🤖 Chatbot: im well\n",
      "🤖 Chatbot: im fine\n",
      "🤖 Chatbot: im well\n",
      "🤖 Chatbot: bad lot people\n",
      "🤖 Chatbot: im fine\n",
      "🤖 Chatbot: cousin went labor baby last week\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import deque\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split into question-answer pairs\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = text.lower().strip()  # Convert to lowercase\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Use SBERT Model for Semantic Understanding\n",
    "sbert_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# Convert questions to embeddings\n",
    "X_embeddings = sbert_model.encode(df[\"clean_question\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# ✅ Step 4: Save Data for Fast Retrieval\n",
    "pickle.dump(X_embeddings, open(\"X_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Training complete. Model saved!\")\n",
    "\n",
    "# ✅ Step 5: Chatbot with Conversational Memory\n",
    "class ChatbotWithMemory:\n",
    "    def __init__(self):\n",
    "        self.sbert_model = sbert_model\n",
    "        self.X_embeddings = pickle.load(open(\"X_embeddings.pkl\", \"rb\"))\n",
    "        self.responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "        self.questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "        self.memory = deque(maxlen=5)  # Store last 5 conversations\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        user_embedding = self.sbert_model.encode([user_input])\n",
    "        similarities = cosine_similarity(user_embedding, self.X_embeddings)\n",
    "        best_match_index = similarities.argmax()\n",
    "        best_response = self.responses[best_match_index]\n",
    "        best_score = similarities[0, best_match_index]\n",
    "\n",
    "        # ✅ Context Handling - Search Memory for Related Responses\n",
    "        for past_input, past_response in self.memory:\n",
    "            if past_input in user_input or user_input in past_response:\n",
    "                return past_response  # Return relevant past response\n",
    "\n",
    "        # ✅ Avoid Irrelevant Responses (Set Threshold)\n",
    "        if best_score < 0.4:\n",
    "            return \"I'm not sure how to answer that. Can you rephrase?\"\n",
    "\n",
    "        # ✅ Save Conversation in Memory\n",
    "        self.memory.append((user_input, best_response))\n",
    "\n",
    "        return best_response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip().lower()\n",
    "            if user_input == \"exit\":\n",
    "                print(\"🤖 Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            best_response = self.get_best_response(user_input)\n",
    "            print(f\"🤖 Chatbot: {best_response}\")\n",
    "\n",
    "# ✅ Step 6: Run Chatbot\n",
    "chatbot = ChatbotWithMemory()\n",
    "chatbot.chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/neha/Library/Python/3.9/lib/python/site-packages (4.48.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading faiss_cpu-1.10.0-cp39-cp39-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp39-cp39-macosx_10_9_universal2.whl size=169877 sha256=9f49c36f6b8e77c9391bebf6835b1fa1690cf10b36c78a7189abf60068409d4d\n",
      "  Stored in directory: /Users/neha/Library/Caches/pip/wheels/f2/7a/49/9bef8878949914ecb90c08fc5bf30a05e17f475fe7e08b63a8\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0 pickle5-0.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers faiss-cpu nltk pandas numpy pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>hi, doing?</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "      <td>problem. been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>problem. been?</td>\n",
       "      <td>i've great. you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "      <td>i've great. you?</td>\n",
       "      <td>i've good. i'm school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                   response                   clean_question  \\\n",
       "0             i'm fine. how about yourself?                       hi, doing?   \n",
       "1       i'm pretty good. thanks for asking.              i'm fine. yourself?   \n",
       "2         no problem. so how have you been?  i'm pretty good. thanks asking.   \n",
       "3          i've been great. what about you?                   problem. been?   \n",
       "4  i've been good. i'm in school right now.                 i've great. you?   \n",
       "\n",
       "                     clean_response  \n",
       "0               i'm fine. yourself?  \n",
       "1   i'm pretty good. thanks asking.  \n",
       "2                    problem. been?  \n",
       "3                  i've great. you?  \n",
       "4  i've good. i'm school right now.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ✅ Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# ✅ Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Show the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# ✅ Load Transformer Model for Sentence Embeddings (Offline)\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# ✅ Function to convert sentences to embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "# ✅ Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([get_embedding(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# ✅ Save Data for Offline Use\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# ✅ Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: i'm fine. yourself?\n",
      "🤖 Chatbot: i'm pretty good. thanks asking.\n",
      "🤖 Chatbot: tall.\n",
      "🤖 Chatbot: nothing, you?\n",
      "🤖 Chatbot: things, besides, polite nosey.\n",
      "🤖 Chatbot: things, besides, polite nosey.\n",
      "🤖 Chatbot: want get guns street.\n",
      "🤖 Chatbot: want get guns street.\n",
      "🤖 Chatbot: next game, definitely there.\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# ✅ Load Saved Data\n",
    "index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))\n",
    "responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "\n",
    "# ✅ Chatbot Memory (Stores Last 5 Conversations)\n",
    "chat_memory = deque(maxlen=5)\n",
    "\n",
    "# ✅ Function to Find Best Response\n",
    "def get_best_response(user_input):\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    _, best_match_index = index.search(user_embedding, 1)\n",
    "    best_response = responses[best_match_index[0][0]]\n",
    "\n",
    "    # ✅ Check Memory for Context\n",
    "    for past_input, past_response in chat_memory:\n",
    "        if past_input in user_input or user_input in past_response:\n",
    "            return past_response  # Return relevant past response\n",
    "\n",
    "    # ✅ Store Conversation in Memory\n",
    "    chat_memory.append((user_input, best_response))\n",
    "\n",
    "    return best_response\n",
    "\n",
    "# ✅ Interactive Chatbot in Jupyter Notebook\n",
    "def chat():\n",
    "    print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"🤖 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = get_best_response(user_input)\n",
    "        print(f\"🤖 Chatbot: {response}\")\n",
    "\n",
    "# ✅ Run Chatbot\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/neha/Library/Python/3.9/lib/python/site-packages (4.48.2)\n",
      "Requirement already satisfied: faiss-cpu in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.10.0)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: pickle5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (0.0.11)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers faiss-cpu nltk pandas numpy pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>hi, doing?</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "      <td>problem. been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>problem. been?</td>\n",
       "      <td>i've great. you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "      <td>i've great. you?</td>\n",
       "      <td>i've good. i'm school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                   response                   clean_question  \\\n",
       "0             i'm fine. how about yourself?                       hi, doing?   \n",
       "1       i'm pretty good. thanks for asking.              i'm fine. yourself?   \n",
       "2         no problem. so how have you been?  i'm pretty good. thanks asking.   \n",
       "3          i've been great. what about you?                   problem. been?   \n",
       "4  i've been good. i'm in school right now.                 i've great. you?   \n",
       "\n",
       "                     clean_response  \n",
       "0               i'm fine. yourself?  \n",
       "1   i'm pretty good. thanks asking.  \n",
       "2                    problem. been?  \n",
       "3                  i've great. you?  \n",
       "4  i've good. i'm school right now.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ✅ Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# ✅ Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Show the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# ✅ Load Transformer Model for Sentence Embeddings (Offline)\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# ✅ Function to convert sentences to embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "# ✅ Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([get_embedding(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# ✅ Save Data for Offline Use\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# ✅ Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>hi, doing?</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>i'm fine. yourself?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i'm pretty good. thanks asking.</td>\n",
       "      <td>problem. been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>problem. been?</td>\n",
       "      <td>i've great. you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "      <td>i've great. you?</td>\n",
       "      <td>i've good. i'm school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                   response                   clean_question  \\\n",
       "0             i'm fine. how about yourself?                       hi, doing?   \n",
       "1       i'm pretty good. thanks for asking.              i'm fine. yourself?   \n",
       "2         no problem. so how have you been?  i'm pretty good. thanks asking.   \n",
       "3          i've been great. what about you?                   problem. been?   \n",
       "4  i've been good. i'm in school right now.                 i've great. you?   \n",
       "\n",
       "                     clean_response  \n",
       "0               i'm fine. yourself?  \n",
       "1   i'm pretty good. thanks asking.  \n",
       "2                    problem. been?  \n",
       "3                  i've great. you?  \n",
       "4  i've good. i'm school right now.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ✅ Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# ✅ Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Show the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ✅ Load a proper embedding model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# ✅ Function to extract embeddings correctly\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0, :].numpy()  # Correct embedding extraction\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# ✅ Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([get_embedding(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# ✅ Save Data for Offline Use\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# ✅ Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/neha/Library/Python/3.9/lib/python/site-packages (4.48.2)\n",
      "Requirement already satisfied: faiss-cpu in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.10.0)\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /Users/neha/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/neha/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: pickle5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (0.0.11)\n",
      "Requirement already satisfied: filelock in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers faiss-cpu nltk pandas numpy pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete! Model saved for offline use.\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import deque\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Use Sentence Transformer for Embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# ✅ Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([sbert_model.encode(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# ✅ Save Embeddings Using FAISS for Fast Retrieval\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# ✅ Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Training complete! Model saved for offline use.\")\n",
    "\n",
    "# ✅ Step 4: Chatbot with Memory\n",
    "class ChatbotWithMemory:\n",
    "    def __init__(self):\n",
    "        self.sbert_model = sbert_model\n",
    "        self.index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))\n",
    "        self.responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "        self.questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "        self.memory = deque(maxlen=10)  # Stores last 10 conversations\n",
    "        self.memory_dict = {}  # Stores user preferences, name, etc.\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        \"\"\"Finds the most relevant response from the dataset using FAISS.\"\"\"\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "        _, best_match_index = self.index.search(user_embedding, 1)\n",
    "        best_response = self.responses[best_match_index[0][0]]\n",
    "\n",
    "        # ✅ Context Handling - Check Memory for Related Responses\n",
    "        for past_input, past_response in self.memory:\n",
    "            if past_input in user_input or user_input in past_response:\n",
    "                return past_response  # Return a relevant past response\n",
    "\n",
    "        # ✅ Store the conversation in memory\n",
    "        self.memory.append((user_input, best_response))\n",
    "\n",
    "        return best_response\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chatbot function.\"\"\"\n",
    "        print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip().lower()\n",
    "            if user_input == \"exit\":\n",
    "                print(\"🤖 Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # ✅ Store User Details Dynamically\n",
    "            if \"my name is\" in user_input:\n",
    "                name = user_input.split(\"my name is\")[-1].strip()\n",
    "                self.memory_dict[\"name\"] = name\n",
    "                print(f\"🤖 Chatbot: Nice to meet you, {name}!\")\n",
    "                continue\n",
    "\n",
    "            elif \"what is my name\" in user_input and \"name\" in self.memory_dict:\n",
    "                print(f\"🤖 Chatbot: Your name is {self.memory_dict['name']}.\")\n",
    "                continue\n",
    "\n",
    "            elif \"i like\" in user_input:\n",
    "                preference = user_input.split(\"i like\")[-1].strip()\n",
    "                self.memory_dict[\"preference\"] = preference\n",
    "                print(f\"🤖 Chatbot: Great! Ill remember that you like {preference}.\")\n",
    "                continue\n",
    "\n",
    "            elif \"what do i like\" in user_input and \"preference\" in self.memory_dict:\n",
    "                print(f\"🤖 Chatbot: You like {self.memory_dict['preference']}!\")\n",
    "                continue\n",
    "\n",
    "            # ✅ Retrieve the Best Response\n",
    "            best_response = self.get_best_response(user_input)\n",
    "\n",
    "            print(f\"🤖 Chatbot: {best_response}\")\n",
    "\n",
    "# ✅ Run Chatbot\n",
    "chatbot = ChatbotWithMemory()\n",
    "chatbot.chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete! Model saved for offline use.\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: I'm not sure how to answer that. Can you ask differently?\n",
      "🤖 Chatbot: I'm not sure how to answer that. Can you ask differently?\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import deque\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Use Sentence Transformer for Embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# ✅ Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([sbert_model.encode(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# ✅ Save Embeddings Using FAISS for Fast Retrieval\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# ✅ Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Training complete! Model saved for offline use.\")\n",
    "\n",
    "# ✅ Step 4: Chatbot with Correct Response Retrieval\n",
    "class ChatbotWithMemory:\n",
    "    def __init__(self):\n",
    "        self.sbert_model = sbert_model\n",
    "        self.index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))\n",
    "        self.responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "        self.questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "        self.memory = deque(maxlen=10)  # Stores last 10 conversations\n",
    "        self.memory_dict = {}  # Stores user preferences, name, etc.\n",
    "        self.last_response = None  # Prevent response repetition\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        \"\"\"Finds the most relevant response using FAISS, ensuring it is meaningful.\"\"\"\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "        distances, best_match_index = self.index.search(user_embedding, 3)  # Get top 3 best matches\n",
    "        possible_responses = [self.responses[i] for i in best_match_index[0]]\n",
    "        scores = distances[0]\n",
    "\n",
    "        # ✅ Ensure meaningful responses (avoid bad matches)\n",
    "        for response, score in zip(possible_responses, scores):\n",
    "            if response != self.last_response and response.strip() != \"\" and score < 0.4:\n",
    "                self.last_response = response\n",
    "                return response\n",
    "\n",
    "        # ✅ If all responses are similar, return a fallback response\n",
    "        return \"I'm not sure how to answer that. Can you ask differently?\"\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chatbot function.\"\"\"\n",
    "        print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip().lower()\n",
    "            if user_input == \"exit\":\n",
    "                print(\"🤖 Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # ✅ Ignore empty or invalid input\n",
    "            if user_input in [\"\", \".\", \"?\", \"!\"]:\n",
    "                print(\"🤖 Chatbot: Could you provide more details?\")\n",
    "                continue\n",
    "\n",
    "            # ✅ Retrieve the Best Response from `dialogs.txt`\n",
    "            best_response = self.get_best_response(user_input)\n",
    "\n",
    "            print(f\"🤖 Chatbot: {best_response}\")\n",
    "\n",
    "# ✅ Run Chatbot\n",
    "chatbot = ChatbotWithMemory()\n",
    "chatbot.chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete! Model saved for offline use.\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: i'm fine. yourself?\n",
      "🤖 Chatbot: Great! I’ll remember that you like .\n",
      "🤖 Chatbot: Nice to meet you, jitesh!\n",
      "🤖 Chatbot: i.\n",
      "🤖 Chatbot: practice every day.\n",
      "🤖 Chatbot: practice every day.\n",
      "🤖 Chatbot: kid.\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import nltk\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import deque\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Use Sentence Transformer for Embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# ✅ Convert all questions to embeddings\n",
    "question_embeddings = np.vstack([sbert_model.encode(q) for q in df[\"clean_question\"]])\n",
    "\n",
    "# ✅ Save Embeddings Using FAISS for Fast Retrieval\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# ✅ Save Data Locally\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Training complete! Model saved for offline use.\")\n",
    "\n",
    "# ✅ Step 4: Chatbot with Memory\n",
    "class ChatbotWithMemory:\n",
    "    def __init__(self):\n",
    "        self.sbert_model = sbert_model\n",
    "        self.index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))\n",
    "        self.responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "        self.questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "        self.memory = deque(maxlen=10)  # Stores last 10 conversations\n",
    "        self.memory_dict = {}  # Stores user preferences, name, etc.\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        \"\"\"Finds the most relevant response from the dataset using FAISS.\"\"\"\n",
    "        user_embedding = sbert_model.encode([user_input])\n",
    "        _, best_match_index = self.index.search(user_embedding, 1)\n",
    "        best_response = self.responses[best_match_index[0][0]]\n",
    "\n",
    "        # ✅ Context Handling - Check Memory for Related Responses\n",
    "        for past_input, past_response in self.memory:\n",
    "            if past_input in user_input or user_input in past_response:\n",
    "                return past_response  # Return a relevant past response\n",
    "\n",
    "        # ✅ Store the conversation in memory\n",
    "        self.memory.append((user_input, best_response))\n",
    "\n",
    "        return best_response\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chatbot function.\"\"\"\n",
    "        print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip().lower()\n",
    "            if user_input == \"exit\":\n",
    "                print(\"🤖 Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # ✅ Store User Details Dynamically\n",
    "            if \"my name is\" in user_input:\n",
    "                name = user_input.split(\"my name is\")[-1].strip()\n",
    "                self.memory_dict[\"name\"] = name\n",
    "                print(f\"🤖 Chatbot: Nice to meet you, {name}!\")\n",
    "                continue\n",
    "\n",
    "            elif \"what is my name\" in user_input and \"name\" in self.memory_dict:\n",
    "                print(f\"🤖 Chatbot: Your name is {self.memory_dict['name']}.\")\n",
    "                continue\n",
    "\n",
    "            elif \"i like\" in user_input:\n",
    "                preference = user_input.split(\"i like\")[-1].strip()\n",
    "                self.memory_dict[\"preference\"] = preference\n",
    "                print(f\"🤖 Chatbot: Great! I’ll remember that you like {preference}.\")\n",
    "                continue\n",
    "\n",
    "            elif \"what do i like\" in user_input and \"preference\" in self.memory_dict:\n",
    "                print(f\"🤖 Chatbot: You like {self.memory_dict['preference']}!\")\n",
    "                continue\n",
    "\n",
    "            # ✅ Retrieve the Best Response\n",
    "            best_response = self.get_best_response(user_input)\n",
    "\n",
    "            print(f\"🤖 Chatbot: {best_response}\")\n",
    "\n",
    "# ✅ Run Chatbot\n",
    "chatbot = ChatbotWithMemory()\n",
    "chatbot.chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3725 lines from the file.\n",
      "hi, how are you doing?\ti'm fine. how about yourself?\n",
      "i'm fine. how about yourself?\ti'm pretty good. thanks for asking.\n",
      "i'm pretty good. thanks for asking.\tno problem. so how have you been?\n",
      "no problem. so how have you been?\ti've been great. what about you?\n",
      "i've been great. what about you?\ti've been good. i'm in school right now.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"  # Adjust if needed\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Check if file has content\n",
    "if not lines:\n",
    "    print(\"❌ Error: The file is empty!\")\n",
    "else:\n",
    "    print(f\"✅ Loaded {len(lines)} lines from the file.\")\n",
    "\n",
    "# ✅ Display a few lines\n",
    "for i in range(5):  # Show first 5 lines\n",
    "    print(lines[i].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              question  \\\n",
      "0               hi, how are you doing?   \n",
      "1        i'm fine. how about yourself?   \n",
      "2  i'm pretty good. thanks for asking.   \n",
      "3    no problem. so how have you been?   \n",
      "4     i've been great. what about you?   \n",
      "\n",
      "                                   response  \n",
      "0             i'm fine. how about yourself?  \n",
      "1       i'm pretty good. thanks for asking.  \n",
      "2         no problem. so how have you been?  \n",
      "3          i've been great. what about you?  \n",
      "4  i've been good. i'm in school right now.  \n"
     ]
    }
   ],
   "source": [
    "# ✅ Check if the dataset is formatted correctly\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Show first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              question  \\\n",
      "0               hi, how are you doing?   \n",
      "1        i'm fine. how about yourself?   \n",
      "2  i'm pretty good. thanks for asking.   \n",
      "3    no problem. so how have you been?   \n",
      "4     i've been great. what about you?   \n",
      "\n",
      "                                   response                   clean_question  \\\n",
      "0             i'm fine. how about yourself?                       hi, doing?   \n",
      "1       i'm pretty good. thanks for asking.              i'm fine. yourself?   \n",
      "2         no problem. so how have you been?  i'm pretty good. thanks asking.   \n",
      "3          i've been great. what about you?                   problem. been?   \n",
      "4  i've been good. i'm in school right now.                 i've great. you?   \n",
      "\n",
      "                     clean_response  \n",
      "0               i'm fine. yourself?  \n",
      "1   i'm pretty good. thanks asking.  \n",
      "2                    problem. been?  \n",
      "3                  i've great. you?  \n",
      "4  i've good. i'm school right now.  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Lowercases and removes stopwords.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Remove empty values\n",
    "df = df.dropna(subset=[\"clean_question\", \"clean_response\"])\n",
    "df = df[df[\"clean_question\"].str.strip() != \"\"]\n",
    "\n",
    "# ✅ Show the final dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS Index trained successfully!\n",
      "🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\n",
      "🤖 Chatbot: I'm not sure how to answer that. Can you rephrase your question?\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ✅ Load a proper model for embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# ✅ Convert all questions to embeddings\n",
    "valid_questions = df[\"clean_question\"].tolist()\n",
    "question_embeddings = np.vstack([sbert_model.encode(q) for q in valid_questions if q.strip() != \"\"])\n",
    "\n",
    "# ✅ Ensure FAISS is trained properly\n",
    "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "index.add(question_embeddings)\n",
    "\n",
    "# ✅ Save Data\n",
    "pickle.dump(index, open(\"faiss_index.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "pickle.dump(valid_questions, open(\"questions.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ FAISS Index trained successfully!\")\n",
    "\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# ✅ Load Stored Data (Fixing File Mode)\n",
    "index = pickle.load(open(\"faiss_index.pkl\", \"rb\"))  # Read mode\n",
    "responses = pickle.load(open(\"responses.pkl\", \"rb\"))  # Read mode\n",
    "questions = pickle.load(open(\"questions.pkl\", \"rb\"))  # ✅ Fixed to \"rb\"\n",
    "\n",
    "# ✅ Chatbot Memory (Stores Last 10 Conversations)\n",
    "chat_memory = deque(maxlen=10)\n",
    "\n",
    "# ✅ Function to Find Best Response\n",
    "def get_best_response(user_input):\n",
    "    user_embedding = sbert_model.encode([user_input])\n",
    "    distances, best_match_index = index.search(user_embedding, 3)  # Get top 3 best matches\n",
    "    possible_responses = [responses[i] for i in best_match_index[0]]\n",
    "    scores = distances[0]\n",
    "\n",
    "    # ✅ Ensure meaningful responses (avoid bad matches)\n",
    "    for response, score in zip(possible_responses, scores):\n",
    "        if response.strip() != \"\" and score < 0.5:  # Adjust threshold if needed\n",
    "            return response\n",
    "\n",
    "    # ✅ If no good match, ask user to clarify\n",
    "    return \"I'm not sure how to answer that. Can you rephrase your question?\"\n",
    "\n",
    "# ✅ Interactive Chatbot in Jupyter Notebook\n",
    "def chat():\n",
    "    print(\"🤖 Chatbot: Hello! Ask me anything. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"🤖 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # ✅ Ignore empty or invalid input\n",
    "        if user_input in [\"\", \".\", \"?\", \"!\"]:\n",
    "            print(\"🤖 Chatbot: Could you provide more details?\")\n",
    "            continue\n",
    "\n",
    "        # ✅ Retrieve the Best Response from `dialogs.txt`\n",
    "        best_response = get_best_response(user_input)\n",
    "\n",
    "        print(f\"🤖 Chatbot: {best_response}\")\n",
    "\n",
    "# ✅ Run Chatbot\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training Chatbot Model...\n",
      "Epoch 1/20 - Loss: 8.3245\n",
      "Epoch 2/20 - Loss: 7.9115\n",
      "Epoch 3/20 - Loss: 7.7390\n",
      "Epoch 4/20 - Loss: 7.1597\n",
      "Epoch 5/20 - Loss: 6.3278\n",
      "Epoch 6/20 - Loss: 5.9846\n",
      "Epoch 7/20 - Loss: 5.1473\n",
      "Epoch 8/20 - Loss: 4.6981\n",
      "Epoch 9/20 - Loss: 3.8446\n",
      "Epoch 10/20 - Loss: 2.4549\n",
      "Epoch 11/20 - Loss: 1.6012\n",
      "Epoch 12/20 - Loss: 1.1400\n",
      "Epoch 13/20 - Loss: 1.0119\n",
      "Epoch 14/20 - Loss: 1.6997\n",
      "Epoch 15/20 - Loss: 0.8047\n",
      "Epoch 16/20 - Loss: 1.0525\n",
      "Epoch 17/20 - Loss: 0.8057\n",
      "Epoch 18/20 - Loss: 1.2940\n",
      "Epoch 19/20 - Loss: 1.7184\n",
      "Epoch 20/20 - Loss: 1.0626\n",
      "✅ Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"dialogs.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Split dataset into questions and responses\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 2: Preprocess Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 3: Convert Text to Vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean_question\"]).toarray()\n",
    "y = df[\"clean_response\"].values\n",
    "\n",
    "# ✅ Step 4: Encode Responses\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# ✅ Save Tokenizer & Label Encoder\n",
    "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(label_encoder, open(\"label_encoder.pkl\", \"wb\"))\n",
    "\n",
    "# ✅ Step 5: Convert to PyTorch Dataset\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = ChatDataset(X, y_encoded)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# ✅ Step 6: Define RNN Model\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x.unsqueeze(1))\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "model = ChatbotModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# ✅ Step 7: Train Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"✅ Training Chatbot Model...\")\n",
    "\n",
    "for epoch in range(20):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/20 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ✅ Save Trained Model\n",
    "torch.save(model.state_dict(), \"chatbot_model.pth\")\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training Chatbot Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 8.3839\n",
      "Epoch 2/10 - Loss: 8.1641\n",
      "Epoch 3/10 - Loss: 7.6838\n",
      "Epoch 4/10 - Loss: 7.1476\n",
      "Epoch 5/10 - Loss: 6.4886\n",
      "Epoch 6/10 - Loss: 5.1924\n",
      "Epoch 7/10 - Loss: 5.3963\n",
      "Epoch 8/10 - Loss: 3.8377\n",
      "Epoch 9/10 - Loss: 3.6412\n",
      "Epoch 10/10 - Loss: 3.6107\n",
      "✅ Model trained and saved successfully!\n",
      "🤖 Chatbot: Hello! I remember past conversations. (Type 'exit' to stop)\n",
      "🤖 Chatbot: one. one that's black.\n",
      "🤖 Chatbot: would date someone even know?\n",
      "🤖 Chatbot: thank you. that's polite ask.\n",
      "🤖 Chatbot: would date someone even know?\n",
      "🤖 Chatbot: would date someone even know?\n",
      "🤖 Chatbot: put it?\n",
      "🤖 Chatbot: put it?\n",
      "🤖 Chatbot: law. can't keep jail forever.\n",
      "🤖 Chatbot: well, dozen large eggs 99 cents.\n",
      "🤖 Chatbot: gravity?\n",
      "🤖 Chatbot: know robber looks like?\n",
      "🤖 Chatbot: gravity?\n",
      "🤖 Chatbot: gravity?\n",
      "🤖 Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Install Required Libraries\n",
    "!pip install nltk torch scikit-learn pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "file_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"  # Ensure you upload this file in Colab\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ✅ Step 2: Process Dataset\n",
    "conversations = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "df = pd.DataFrame(conversations, columns=[\"question\", \"response\"])\n",
    "\n",
    "# ✅ Step 3: Clean Text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df[\"clean_question\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"clean_response\"] = df[\"response\"].apply(clean_text)\n",
    "\n",
    "# ✅ Step 4: Convert Text to Vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean_question\"]).toarray()\n",
    "y = df[\"clean_response\"].values\n",
    "\n",
    "# ✅ Encode Responses\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# ✅ Save Tokenizer & Encoder\n",
    "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(label_encoder, open(\"label_encoder.pkl\", \"wb\"))\n",
    "\n",
    "# ✅ Step 5: Convert to PyTorch Dataset\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = ChatDataset(X, y_encoded)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# ✅ Step 6: Define LSTM Model\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x.unsqueeze(1))\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "model = ChatbotModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# ✅ Step 7: Train Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"✅ Training Chatbot Model...\")\n",
    "\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/10 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ✅ Save Trained Model\n",
    "torch.save(model.state_dict(), \"chatbot_model.pth\")\n",
    "pickle.dump(df[\"clean_question\"].tolist(), open(\"questions.pkl\", \"wb\"))\n",
    "pickle.dump(df[\"clean_response\"].tolist(), open(\"responses.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Model trained and saved successfully!\")\n",
    "\n",
    "# ✅ Step 8: Load Model and Chat with AI\n",
    "vectorizer = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n",
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", \"rb\"))\n",
    "questions = pickle.load(open(\"questions.pkl\", \"rb\"))\n",
    "responses = pickle.load(open(\"responses.pkl\", \"rb\"))\n",
    "\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x.unsqueeze(1))\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "model = ChatbotModel(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(\"chatbot_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# ✅ Memory for Conversation Context\n",
    "memory = deque(maxlen=5)  # Stores the last 5 messages for context\n",
    "\n",
    "def get_best_response(user_input):\n",
    "    \"\"\"Predicts the best response using the trained model.\"\"\"\n",
    "    input_vector = vectorizer.transform([\" \".join(memory) + \" \" + user_input]).toarray()\n",
    "    input_tensor = torch.tensor(input_vector, dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    predicted_index = torch.argmax(output).item()\n",
    "    return responses[predicted_index]\n",
    "\n",
    "def chat():\n",
    "    print(\"🤖 Chatbot: Hello! I remember past conversations. (Type 'exit' to stop)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip().lower()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"🤖 Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # ✅ Store previous messages in memory\n",
    "        memory.append(user_input)\n",
    "\n",
    "        # ✅ Generate AI response\n",
    "        best_response = get_best_response(user_input)\n",
    "\n",
    "        print(f\"🤖 Chatbot: {best_response}\")\n",
    "\n",
    "# ✅ Run Chatbot\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class SuperAdvancedChatbot:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.qa_df = self.load_dataset()\n",
    "        self.vectorizer, self.question_vectors = self.train_vectorizer()\n",
    "        self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.embeddings = self.sbert_model.encode(self.qa_df[\"question\"].tolist(), convert_to_tensor=True)\n",
    "        self.conversation_history = []\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "        with open(self.dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "        qa_pairs = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
    "        return pd.DataFrame(qa_pairs, columns=[\"question\", \"answer\"])\n",
    "\n",
    "    def train_vectorizer(self):\n",
    "        \"\"\"Fits a TF-IDF vectorizer on the dataset's questions.\"\"\"\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        question_vectors = vectorizer.fit_transform(self.qa_df[\"question\"])\n",
    "        return vectorizer, question_vectors\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Tokenizes, removes stopwords, and performs lemmatization for better understanding.\"\"\"\n",
    "        doc = nlp(text.lower())\n",
    "        filtered_tokens = [token.lemma_ for token in doc if token.is_alpha and token.text not in self.stop_words]\n",
    "        return \" \".join(filtered_tokens)\n",
    "\n",
    "    def get_best_response(self, user_input):\n",
    "        \"\"\"Finds the best response using sentence embeddings and remembers the conversation.\"\"\"\n",
    "        user_input = self.preprocess_text(user_input)\n",
    "        self.conversation_history.append(user_input)\n",
    "\n",
    "        user_embedding = self.sbert_model.encode([user_input], convert_to_tensor=True)\n",
    "        similarities = cosine_similarity(user_embedding.cpu().numpy(), self.embeddings.cpu().numpy())\n",
    "        best_match_idx = np.argmax(similarities)\n",
    "        \n",
    "        if similarities[0, best_match_idx] < 0.4:\n",
    "            return random.choice([\"I'm not sure I understand.\", \"Can you rephrase that?\", \"That's interesting! Tell me more.\"])\n",
    "        \n",
    "        return self.qa_df.iloc[best_match_idx][\"answer\"]\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"Runs an interactive chatbot with deep learning-based understanding and memory.\"\"\"\n",
    "        print(\"Super Advanced Chatbot is ready! Type 'exit' to quit.\")\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            response = self.get_best_response(user_input)\n",
    "            print(f\"Chatbot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = SuperAdvancedChatbot(\"dialogs.txt\")\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/neha/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/neha/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neha/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/neha/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('wordnet')  # Lemmatizer support\n",
    "nltk.download('averaged_perceptron_tagger')  # POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/neha/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# Run Training and Chat\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     model, word2idx, idx2word \u001b[39m=\u001b[39m train_model(dataset_path)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     chat(model, word2idx, idx2word)\n",
      "\u001b[1;32m/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb Cell 35\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(dataset_path, epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     dataset \u001b[39m=\u001b[39m ChatDataset(dataset_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mx))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     model \u001b[39m=\u001b[39m ChatbotModel(\u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mword2idx))\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2idx \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m<SOS>\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m<EOS>\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m}  \u001b[39m# Special tokens\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midx2word \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m<SOS>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m<EOS>\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_vocab()\n",
      "\u001b[1;32m/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb Cell 35\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m word_freq \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mfor\u001b[39;00m q, a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpairs:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_tokenize(q) \u001b[39m+\u001b[39m word_tokenize(a):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         word_freq[word\u001b[39m.\u001b[39mlower()] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neha/Documents/chatbot/Untitledlittle-1.ipynb#X43sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m sorted_words \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(word_freq\u001b[39m.\u001b[39mkeys(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39m-\u001b[39mword_freq[x])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/neha/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/Users/neha/Documents/chatbot/dialogs.txt\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Check path.\")\n",
    "\n",
    "# Load and preprocess data\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        self.pairs = self.load_dataset(dataset_path)\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}  # Special tokens\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n",
    "        self.build_vocab()\n",
    "\n",
    "    def load_dataset(self, dataset_path):\n",
    "        with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = [line.strip() for line in file.readlines() if \"\\t\" in line]\n",
    "        return [tuple(line.split(\"\\t\")) for line in lines]\n",
    "\n",
    "    def build_vocab(self):\n",
    "        word_freq = defaultdict(int)\n",
    "        for q, a in self.pairs:\n",
    "            for word in word_tokenize(q) + word_tokenize(a):\n",
    "                word_freq[word.lower()] += 1\n",
    "\n",
    "        sorted_words = sorted(word_freq.keys(), key=lambda x: -word_freq[x])\n",
    "        for i, word in enumerate(sorted_words, start=3):  # Start indexing from 3\n",
    "            self.word2idx[word] = i\n",
    "            self.idx2word[i] = word\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        return [self.word2idx.get(word.lower(), 0) for word in word_tokenize(sentence)] + [2]  # Add <EOS>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q, a = self.pairs[idx]\n",
    "        return torch.tensor(self.encode_sentence(q)), torch.tensor(self.encode_sentence(a))\n",
    "\n",
    "# Define LSTM Chatbot Model\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        _, hidden_state = self.encoder(embedded, hidden_state)\n",
    "        decoder_output, _ = self.decoder(embedded, hidden_state)\n",
    "        return self.fc(decoder_output)\n",
    "\n",
    "# Training function\n",
    "def train_model(dataset_path, epochs=5, batch_size=4, learning_rate=0.001):\n",
    "    dataset = ChatDataset(dataset_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: zip(*x))\n",
    "\n",
    "    model = ChatbotModel(len(dataset.word2idx)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}...\")\n",
    "        total_loss = 0\n",
    "        for q_batch, a_batch in dataloader:\n",
    "            q_batch = torch.nn.utils.rnn.pad_sequence(q_batch, batch_first=True, padding_value=0).to(device)\n",
    "            a_batch = torch.nn.utils.rnn.pad_sequence(a_batch, batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(q_batch)\n",
    "\n",
    "            loss = criterion(output.view(-1, len(dataset.word2idx)), a_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"chatbot_lstm.pth\")\n",
    "    print(\"Model training complete. Model saved.\")\n",
    "    return model, dataset.word2idx, dataset.idx2word\n",
    "\n",
    "# Chat function\n",
    "def chat(model, word2idx, idx2word):\n",
    "    print(\"Chatbot is ready! Type 'exit' to stop.\")\n",
    "\n",
    "    model.eval()\n",
    "    memory = []  # Stores last few interactions\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        tokens = [word2idx.get(word.lower(), 0) for word in word_tokenize(user_input)] + [2]  # Add <EOS>\n",
    "        input_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        response_tokens = torch.argmax(output, dim=-1).squeeze().tolist()\n",
    "        response = \" \".join(idx2word.get(token, \"?\") for token in response_tokens if token not in {1, 2})\n",
    "\n",
    "        memory.append((user_input, response))\n",
    "        if len(memory) > 5:  # Limit memory to last 5 interactions\n",
    "            memory.pop(0)\n",
    "\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Run Training and Chat\n",
    "if __name__ == \"__main__\":\n",
    "    model, word2idx, idx2word = train_model(dataset_path)\n",
    "    chat(model, word2idx, idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
